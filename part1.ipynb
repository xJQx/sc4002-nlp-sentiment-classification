{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Preparing Word Embeddings\n",
    "\n",
    "This jupyter notebook outlines the our process for preparing word embeddings for part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'tokens', 'original_len'],\n",
       "    num_rows: 8530\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.text import tokenize\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "train_dataset = tokenize(train_dataset)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load word2vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have decided to use Google's word2vec model.\n",
    "\n",
    "It was trained on a part of the Google News dataset (about 100 billion words) and the model contains 300-dimensional vectors for 3 million words and phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "import os\n",
    "\n",
    "w2v_model_path = \"models/word2vec-google-news-300\"\n",
    "\n",
    "# Download pretrained embeddings model if haven't done so\n",
    "if not os.path.exists(w2v_model_path):\n",
    "    # Takes around 7mins\n",
    "    w2v_model = gensim.downloader.load(\"word2vec-google-news-300\")\n",
    "    w2v_model.save(w2v_model_path)\n",
    "\n",
    "    # Alternatively, download from the link below\n",
    "    # w2v_model = gensim.models.KeyedVectors.load_word2vec_format('model\\GoogleNews-vectors-negative300.bin\\GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    # download the pretrained model from https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g (take around 1.5GB)\n",
    "\n",
    "w2v_model = gensim.models.KeyedVectors.load(w2v_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v_model.index_to_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Size of vocabulary from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16332"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_train = set([word for sentence in train_dataset[\"tokens\"] for word in sentence])\n",
    "\n",
    "len(vocab_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Number of OOV (out-of-vocabulary) words in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1446\n",
      "['javier', 'seagal', 'mamet', 'contemplarse', 'gabriele', 'zucker', '<PAD>', 'bugsy', 'demencial', 'pompeo', 'kosashvili', 'curiosa', 'bigelow', 'unsuspenseful', 'astoria', 'rechy', 'statham', 'ladr', 'rymer', 'verdadera']\n"
     ]
    }
   ],
   "source": [
    "oov_words = [word for word in vocab_train if word not in w2v_model]\n",
    "\n",
    "print(len(oov_words))\n",
    "print(oov_words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Handling OOV words\n",
    "\n",
    "To handle OOV words, we have considered various options:\n",
    "- **Global Average:** mean of all embeddings in the w2v model\n",
    "- **Contextual Average:** mean of embeddings of the surrounding words in the training data\n",
    "- **Similar Words:** generate an embedding by finding the closest match in w2v vocab\n",
    "\n",
    "We have decided that **contextual average** is the best approach as the meaning of words are usually contextual, hence the contextual average approach would be better able to approximate the OOV word's meaning, making it more versatile.\n",
    "\n",
    "On the other hand, the similar words approach might not work well for completely novel words like nouns or names. Since our OOV words in the training data contains a large number of nouns, this approach might not be as suitable for our use case.\n",
    "\n",
    "The global average method is not good as it is just assigns all OOV words.\n",
    "\n",
    "**Selected approach: Contextual Average**\n",
    "\n",
    "> We have implemented the contextual average approach and they can be found in [utils/text.py](utils/text.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOV embedding size: 300\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "from utils.text import get_context_average_embedding\n",
    "\n",
    "sentence_tokens = [\"this\", \"is\", \"an\", \"example\", \"with\", \"oovword\"]\n",
    "oov_token = \"oovword\"\n",
    "embedding = get_context_average_embedding(sentence_tokens, oov_token, w2v_model)\n",
    "\n",
    "print(\"OOV embedding size:\", embedding.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare word embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embedding_dim = w2v_model.vector_size\n",
    "vocab_size = len(vocab_train)\n",
    "\n",
    "# Initialize embedding matrix with zeros (Add one for <PAD>).\n",
    "embedding_matrix = np.zeros((vocab_size + 1, embedding_dim))\n",
    "\n",
    "# Word to index dictionary for easy lookup.\n",
    "index_from_word = {word: i + 1 for i, word in enumerate(vocab_train)}\n",
    "index_from_word[\"<PAD>\"] = 0\n",
    "\n",
    "# Populate embedding matrix.\n",
    "for word, i in index_from_word.items():\n",
    "    if word in w2v_model.key_to_index:\n",
    "        embedding_matrix[i] = w2v_model[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export Embedding Matrix & vocab_index_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "embedding_path = Path(\"models/embedding_matrix.npy\")\n",
    "index_from_word_path = Path(\"models/index_from_word.json\")\n",
    "\n",
    "np.save(embedding_path, embedding_matrix)\n",
    "\n",
    "with index_from_word_path.open(\"w\") as f:\n",
    "    json.dump(index_from_word, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
