{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Import dataset from part 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 8530\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Process text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/yuri/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to /home/yuri/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/yuri/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from utils.text import tokenize\n",
    "\n",
    "train_dataset = tokenize(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparing Word Embeddings\n",
    "\n",
    "- using `Word2Vec`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load a pretrained word2vec model (trained on Google News dataset contained about 100 billion words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "import os\n",
    "\n",
    "w2v_model_path = \"models/word2vec-google-news-300\"\n",
    "\n",
    "# Download pretrained embeddings model if haven't done so\n",
    "if not os.path.exists(w2v_model_path):\n",
    "    # Takes around 7mins\n",
    "    w2v_model = gensim.downloader.load(\"word2vec-google-news-300\")\n",
    "    w2v_model.save(w2v_model_path)\n",
    "\n",
    "    # Alternatively, download from the link below\n",
    "    # w2v_model = gensim.models.KeyedVectors.load_word2vec_format('model\\GoogleNews-vectors-negative300.bin\\GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    # download the pretrained model from https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g (take around 1.5GB)\n",
    "\n",
    "w2v_model = gensim.models.KeyedVectors.load(w2v_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v_model.index_to_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) size of vocabulary from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16331"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_train = set([word for sentence in train_dataset[\"tokens\"] for word in sentence])\n",
    "\n",
    "len(vocab_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) number of OOV (out-of-vocabulary)\n",
    "- (those words appeared in the training data but not in the Word2vec dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1445\n",
      "['kosminsky', 'desplechin', 'reeses', 'alcatraz', 'weissman', 'esteticamente', 'premissa', 'bugsy', 'manhunter', 'khouri', 'mctiernan', 'francamente', 'kubrick', 'glamour', 'pianista', 'shamu', 'vittorio', 'weigel', 'clements', 'mulan']\n"
     ]
    }
   ],
   "source": [
    "oov_words = [word for word in vocab_train if word not in w2v_model]\n",
    "\n",
    "print(len(oov_words))\n",
    "print(oov_words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) initialize an embedding matrix\n",
    "- handling of OOV words: using the mean vector from the pretrained w2v vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embedding_dim = w2v_model.vector_size  # 300\n",
    "\n",
    "# Initialize embedding matrix (train_data_vocab_size+1 X embedding dimension)\n",
    "# `+1` for OOV words in val and test set\n",
    "embedding_matrix = np.zeros((vocab_size_train + 1, embedding_dim))\n",
    "\n",
    "# Vocab-to-index dict\n",
    "vocab_index_train = {vocab: i + 1 for i, vocab in enumerate(vocab_train)}\n",
    "vocab_index_train[\"\"] = 0  # For OOV words in val and test set\n",
    "\n",
    "# Mean vector of the pretrained w2v\n",
    "w2v_vectors = np.array(\n",
    "    [w2v_model[vocab] for vocab in w2v_model.index_to_key]\n",
    ")\n",
    "mean_vector = np.mean(w2v_vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in embedding matrix\n",
    "for vocab in vocab_train:\n",
    "    # Assign mean vector for OOV words\n",
    "    if vocab in oov_words:\n",
    "        embedding_matrix[vocab_index_train[vocab]] = mean_vector\n",
    "    else:\n",
    "        embedding_matrix[vocab_index_train[vocab]] = w2v_model[vocab]\n",
    "\n",
    "# Assign mean vector for OOV words in val and test set\n",
    "embedding_matrix[vocab_index_train[\"\"]] = mean_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export Embedding Matrix & vocab_index_train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving object to local...\n",
      "Object saved to local!\n",
      "Saving object to local...\n",
      "Object saved to local!\n"
     ]
    }
   ],
   "source": [
    "from utils.file import save_to_local_file\n",
    "from pathlib import Path\n",
    "\n",
    "embedding_path = Path(\"models/embedding_matrix.pckl\")\n",
    "vocab_to_index_path = Path(\"models/embedding_matrix_train_dataset_vocab_to_index.pckl\")\n",
    "\n",
    "save_to_local_file(embedding_path, embedding_matrix)\n",
    "save_to_local_file(vocab_to_index_path, vocab_index_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
