{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Import dataset from part 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 8530\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Process text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/yuri/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to /home/yuri/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/yuri/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from utils.text import tokenize\n",
    "\n",
    "train_dataset = tokenize(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparing Word Embeddings\n",
    "\n",
    "- using `Word2Vec`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load a pretrained word2vec model (trained on Google News dataset contained about 100 billion words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "import os\n",
    "\n",
    "w2v_model_path = \"models/word2vec-google-news-300\"\n",
    "\n",
    "# Download pretrained embeddings model if haven't done so\n",
    "if not os.path.exists(w2v_model_path):\n",
    "    # Takes around 7mins\n",
    "    w2v_model = gensim.downloader.load(\"word2vec-google-news-300\")\n",
    "    w2v_model.save(w2v_model_path)\n",
    "\n",
    "    # Alternatively, download from the link below\n",
    "    # w2v_model = gensim.models.KeyedVectors.load_word2vec_format('model\\GoogleNews-vectors-negative300.bin\\GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    # download the pretrained model from https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g (take around 1.5GB)\n",
    "\n",
    "w2v_model = gensim.models.KeyedVectors.load(w2v_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v_model.index_to_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) size of vocabulary from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16331"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_train = set([word for sentence in train_dataset[\"tokens\"] for word in sentence])\n",
    "\n",
    "len(vocab_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) number of OOV (out-of-vocabulary)\n",
    "- (those words appeared in the training data but not in the Word2vec dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1445\n",
      "['kosminsky', 'desplechin', 'reeses', 'alcatraz', 'weissman', 'esteticamente', 'premissa', 'bugsy', 'manhunter', 'khouri', 'mctiernan', 'francamente', 'kubrick', 'glamour', 'pianista', 'shamu', 'vittorio', 'weigel', 'clements', 'mulan']\n"
     ]
    }
   ],
   "source": [
    "oov_words = [word for word in vocab_train if word not in w2v_model]\n",
    "\n",
    "print(len(oov_words))\n",
    "print(oov_words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Handling OOV words\n",
    "\n",
    "To handle OOV words, we have considered various options:\n",
    "- **Global Average:** mean of all embeddings in the w2v model\n",
    "- **Contextual Average:** mean of embeddings of the surrounding words in the training data\n",
    "- **Similar Words:** generate an embedding by finding the closest match in w2v vocab\n",
    "\n",
    "We have decided that **contextual average** is the best approach as the meaning of words are usually contextual, hence the contextual average approach would be better able to approximate the OOV word's meaning, making it more versatile.\n",
    "\n",
    "On the other hand, the similar words approach might not work well for completely novel words like nouns or names. Since our OOV words in the training data contains a large number of nouns, this approach might not be as suitable for our use case.\n",
    "\n",
    "The global average method is not good as it is just assigns all OOV words.\n",
    "\n",
    "**Selected approach: Contextual Average**\n",
    "\n",
    "> We have implemented the contextual average approach and they can be found in [utils/text.py](utils/text.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOV embedding size: 300\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "from utils.text import get_context_average_embedding\n",
    "\n",
    "sentence_tokens = [\"this\", \"is\", \"an\", \"example\", \"with\", \"oovword\"]\n",
    "oov_token = \"oovword\"\n",
    "embedding = get_context_average_embedding(sentence_tokens, oov_token, w2v_model)\n",
    "\n",
    "print(\"OOV embedding size:\", embedding.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the word embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embedding_dim = w2v_model.vector_size\n",
    "vocab_size = len(vocab_train)\n",
    "\n",
    "# Initialize embedding matrix with zeros.\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# Word to index dictionary for easy lookup.\n",
    "index_from_word = {word: i for i, word in enumerate(vocab_train)}\n",
    "\n",
    "# Populate embedding matrix.\n",
    "for word, i in index_from_word.items():\n",
    "    if word in w2v_model.key_to_index:\n",
    "        embedding_matrix[i] = w2v_model[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export Embedding Matrix & vocab_index_train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving object to local...\n",
      "Object saved to local!\n",
      "Saving object to local...\n",
      "Object saved to local!\n"
     ]
    }
   ],
   "source": [
    "from utils.file import save_to_local_file\n",
    "from pathlib import Path\n",
    "\n",
    "embedding_path = Path(\"models/embedding_matrix.pckl\")\n",
    "vocab_to_index_path = Path(\"models/embedding_matrix_train_dataset_vocab_to_index.pckl\")\n",
    "\n",
    "save_to_local_file(embedding_path, embedding_matrix)\n",
    "save_to_local_file(vocab_to_index_path, vocab_index_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
