{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove Dataset Preparation\n",
    "\n",
    "This notebook handles the import of GLoVe model and the corresponding logic to build an embedding matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Glove Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dm/kpwksylj6fz28f7x6l93h4540000gn/T/ipykernel_34276/2052903266.py:10: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_input_file, word2vec_output_file)\n"
     ]
    }
   ],
   "source": [
    "## https://nlp.stanford.edu/projects/glove/\n",
    "## Download from this link, select glove6B.zip, unzip and place in models/ directory\n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Convert GloVe format to word2vec format\n",
    "glove_input_file = 'models/glove.6B.100d.txt'\n",
    "word2vec_output_file = 'models/glove.6B.100d.word2vec.txt'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "\n",
    "# Load the converted model\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Check Glove number of data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.index_to_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juinlee/Documents/GitHub/sc4002-nlp-sentiment-classification/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /Users/juinlee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/juinlee/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/juinlee/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from utils.text import tokenize\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "train_dataset = tokenize(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Word Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.58165002  0.73010498  0.31462398  0.49088001 -1.30036497  0.18174499\n",
      "  0.36901999 -0.25784999 -0.00873     0.482885    0.94368005  0.179148\n",
      " -0.18084499  0.25459     0.51633501 -0.28136    -0.10804     0.0409885\n",
      "  0.02675501  0.73750001  0.38751501 -0.116468   -0.13833001 -0.04469065\n",
      "  0.00855999  1.08362508  0.01229     0.17734501  0.302495   -0.94288999\n",
      "  0.1841445   0.33857     0.21223499 -0.24423     0.56564498  0.84698999\n",
      "  0.59086001  0.94275498  0.1679      0.47887501  0.99735498 -0.78103501\n",
      " -0.57097    -0.95722502  0.02849    -0.65161002  0.093355    0.34443551\n",
      " -0.101515   -0.078851    0.2157      0.17446475 -0.49163499 -0.31758499\n",
      " -1.19385004  0.14773001 -0.22755501  0.29258499  0.136235    0.42129499\n",
      " -0.60543501 -0.21785501 -0.98325002 -0.35978502  1.30250001  0.57874501\n",
      " -0.21573301 -0.13982449  1.10615003  0.33975002 -0.94596493  0.2096225\n",
      "  0.12854199  0.12942749  0.33677    -0.70151997 -0.0560895  -0.376095\n",
      " -0.22258501 -1.44799995 -0.06923    -1.09092999 -0.045155   -0.48834002\n",
      "  0.055558    0.75654995  0.238065   -0.89487004  0.68230999 -0.145577\n",
      " -0.119738    0.414175   -0.36186001  0.18700349 -0.079975   -0.33380499\n",
      "  0.116767    0.051263   -0.140515   -0.84779501]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from utils.text import get_context_average_embedding\n",
    "\n",
    "vocab_train = set([word for sentence in train_dataset[\"tokens\"] for word in sentence])\n",
    "oov_words = set([word for word in vocab_train if word not in model])\n",
    "\n",
    "embedding_dim = model.vector_size\n",
    "\n",
    "# Initialize embedding matrix with zeros (Add 2 for <PAD> and <UNK>).\n",
    "offset = 2\n",
    "embedding_matrix = np.zeros((len(vocab_train) + offset, embedding_dim))\n",
    "\n",
    "# Word to index dictionary for easy lookup.\n",
    "index_from_word = {word: i + offset for i, word in enumerate(vocab_train)}\n",
    "index_from_word[\"<PAD>\"] = 0\n",
    "index_from_word[\"<UNK>\"] = 1\n",
    "\n",
    "# Mean vector of the pretrained GloVe embeddings.\n",
    "vectors = np.array([model[vocab] for vocab in model.index_to_key])\n",
    "mean_vector = np.mean(vectors, axis=0)\n",
    "\n",
    "# Populate embedding matrix with known words.\n",
    "for word, i in index_from_word.items():\n",
    "    if word in model:\n",
    "        embedding_matrix[i] = model[word]\n",
    "\n",
    "\n",
    "# Populate OOV words with context average embedding.\n",
    "oov_words_map = {}\n",
    "for sentence in train_dataset[\"tokens\"]:\n",
    "    for word in sentence:\n",
    "        if word in oov_words:\n",
    "            if word not in oov_words_map:\n",
    "                oov_words_map[word] = 0\n",
    "            oov_words_map[word] += 1\n",
    "            embedding_matrix[index_from_word[word]] += get_context_average_embedding(word, sentence, model)\n",
    "\n",
    "for word in oov_words_map:\n",
    "    embedding_matrix[index_from_word[word]] /= oov_words_map[word]\n",
    "            \n",
    "embedding_matrix[0] = mean_vector\n",
    "embedding_matrix[1] = mean_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Glove Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "embedding_path = Path(\"models/glove_matrix.npy\")\n",
    "index_from_word_path = Path(\"models/glove_index.json\")\n",
    "\n",
    "np.save(embedding_path, embedding_matrix)\n",
    "\n",
    "with index_from_word_path.open(\"w\") as f:\n",
    "    json.dump(index_from_word, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
