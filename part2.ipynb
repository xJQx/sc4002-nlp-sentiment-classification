{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Embeddings and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading object from local...\n",
      "Object loaded from local!\n",
      "Loading object from local...\n",
      "Object loaded from local!\n"
     ]
    }
   ],
   "source": [
    "# Import Embedding Matrix and Embedding Matrix's Train dataset vocab to index dictionary\n",
    "\n",
    "from utils.file import load_from_local_file\n",
    "\n",
    "embedding_matrix = load_from_local_file(\"models/embedding_matrix.pckl\")\n",
    "embedding_matrix_train_dataset_vocab_to_index: dict = load_from_local_file(\"models/embedding_matrix_train_dataset_vocab_to_index.pckl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the rock is destined to be the 21st century's ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the gorgeously elaborate continuation of \" the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>effective but too-tepid biopic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if you sometimes like to go to the movies to h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>emerges as something rare , an issue movie tha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  the rock is destined to be the 21st century's ...      1\n",
       "1  the gorgeously elaborate continuation of \" the...      1\n",
       "2                     effective but too-tepid biopic      1\n",
       "3  if you sometimes like to go to the movies to h...      1\n",
       "4  emerges as something rare , an issue movie tha...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"datasets/train.csv\")\n",
    "val_df = pd.read_csv(\"datasets/val.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_SPACE = {\n",
    "  \"batch_size\": [50, 100, 200],\n",
    "  \"learning_rate\": [0.005, 0.01, 0.025, 0.05],\n",
    "  \"optimizer_name\": [\"SGD\", \"Adagrad\", \"Adam\", \"RMSprop\"],\n",
    "\n",
    "  # RNN Model Parameters\n",
    "  \"hidden_dim\": [16, 32, 64],\n",
    "  \"num_layers\": [2, 4, 8, 16]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Toh Jing\n",
      "[nltk_data]     Qiang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to C:\\Users\\Toh Jing\n",
      "[nltk_data]     Qiang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Toh Jing\n",
      "[nltk_data]     Qiang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from models.RNN import RNN\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from solver import train, plot_loss_acc_graph\n",
    "from utils.custom_dataset import TextDataset\n",
    "\n",
    "def train_rnn_model_with_parameters(\n",
    "    batch_size: int,\n",
    "    learning_rate: float,\n",
    "    optimizer_name: str,\n",
    "    hidden_dim: int,\n",
    "    num_layers: int,\n",
    "):\n",
    "  # Model\n",
    "  model_rnn = RNN(\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    output_dim=2\n",
    "  )\n",
    "\n",
    "  ########################\n",
    "  ###### Parameters ######\n",
    "  ########################\n",
    "  batch_size = batch_size\n",
    "  max_epochs = 10_000\n",
    "\n",
    "  # SGD Optimizer\n",
    "  learning_rate = learning_rate\n",
    "  match optimizer_name:\n",
    "    case \"SGD\":\n",
    "      optimizer = torch.optim.SGD(model_rnn.parameters(), lr=learning_rate)\n",
    "    case \"Adagrad\":\n",
    "      optimizer = torch.optim.Adagrad(model_rnn.parameters(), lr=learning_rate)\n",
    "    case \"Adam\":\n",
    "      optimizer = torch.optim.Adam(model_rnn.parameters(), lr=learning_rate)\n",
    "    case \"RMSprop\":\n",
    "      optimizer = torch.optim.RMSprop(model_rnn.parameters(), lr=learning_rate)\n",
    "    case _:\n",
    "      raise Exception(\"Invalid optimizer name!\")\n",
    "\n",
    "  # Cross Entropy Loss \n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "  ########################\n",
    "  ######## Dataset #######\n",
    "  ########################\n",
    "  train_dataset = TextDataset(\n",
    "    dataframe=train_df,\n",
    "    max_len=train_df[\"text\"].str.split().apply(len).max(),\n",
    "    embedding_matrix_vocab_to_index=embedding_matrix_train_dataset_vocab_to_index\n",
    "  )\n",
    "  train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "  val_dataset = TextDataset(\n",
    "    dataframe=val_df,\n",
    "    max_len=val_df[\"text\"].str.split().apply(len).max(),\n",
    "    embedding_matrix_vocab_to_index=embedding_matrix_train_dataset_vocab_to_index\n",
    "  )\n",
    "  val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "  ########################\n",
    "  ######### Train ########\n",
    "  ########################\n",
    "  model, avg_train_loss, avg_train_acc, avg_val_loss, avg_val_acc, num_of_epochs = train(\n",
    "    model=model_rnn,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    max_epoch=max_epochs\n",
    "  )\n",
    "\n",
    "  ########################\n",
    "  ######### Plot #########\n",
    "  ########################\n",
    "  subtitle = f\"batch_size_{batch_size}; lr_{learning_rate}; optimizer_{optimizer_name}; hidden_dim_{hidden_dim}; num_layers_{num_layers}\"\n",
    "  save_filename_prefix = f\"rnn/batch_size_{batch_size}-lr_{learning_rate}-optimizer_{optimizer_name}-hidden_dim_{hidden_dim}-num_layers_{num_layers}\"\n",
    "  \n",
    "  # Plot Train Loss and Accuracy Graph\n",
    "  plot_loss_acc_graph(\n",
    "    loss_list=avg_train_loss, \n",
    "    acc_list=avg_train_acc, \n",
    "    dataset_type=\"train\",\n",
    "    subtitle=subtitle,\n",
    "    save_filename_prefix=save_filename_prefix,\n",
    "    display=False\n",
    "  )\n",
    "\n",
    "  # Plot Validation Loss and Accuracy Graph\n",
    "  plot_loss_acc_graph(\n",
    "    loss_list=avg_val_loss, \n",
    "    acc_list=avg_val_acc, \n",
    "    dataset_type=\"val\",\n",
    "    subtitle=subtitle,\n",
    "    save_filename_prefix=save_filename_prefix,\n",
    "    display=False\n",
    "  )\n",
    "\n",
    "  ########################\n",
    "  ##### Return Value #####\n",
    "  ########################\n",
    "  configuration_results = {\n",
    "    \"batch_size\": batch_size,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"optimizer_name\": optimizer_name,\n",
    "\n",
    "    # RNN Model Parameters\n",
    "    \"hidden_dim\": hidden_dim,\n",
    "    \"num_layers\": num_layers,\n",
    "\n",
    "    # Model performance\n",
    "    \"train_loss\": avg_train_loss[-1],\n",
    "    \"train_accuracy\": avg_train_acc[-1],\n",
    "    \"val_loss\": avg_val_loss[-1],\n",
    "    \"val_accuracy\": avg_val_acc[-1],\n",
    "\n",
    "    # Epoch Number\n",
    "    \"num_of_epochs\": num_of_epochs\n",
    "  }\n",
    "  return configuration_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- batch_size_50; lr_0.005; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Train): 100%|██████████| 171/171 [00:07<00:00, 23.60it/s, acc=0.686, loss=0.616]\n",
      "Epoch 1 (Val): 100%|██████████| 22/22 [00:00<00:00, 44.91it/s, acc=0.515, loss=0.767]\n",
      "Epoch 2 (Train): 100%|██████████| 171/171 [00:13<00:00, 12.91it/s, acc=0.616, loss=0.656]\n",
      "Epoch 2 (Val): 100%|██████████| 22/22 [00:01<00:00, 17.34it/s, acc=0.515, loss=0.781]\n",
      "Epoch 3 (Train): 100%|██████████| 171/171 [00:13<00:00, 13.09it/s, acc=0.61, loss=0.664] \n",
      "Epoch 3 (Val): 100%|██████████| 22/22 [00:00<00:00, 42.98it/s, acc=0.515, loss=0.781]\n",
      "Epoch 4 (Train): 100%|██████████| 171/171 [00:06<00:00, 26.26it/s, acc=0.604, loss=0.666]\n",
      "Epoch 4 (Val): 100%|██████████| 22/22 [00:00<00:00, 37.23it/s, acc=0.515, loss=0.779]\n",
      "Epoch 5 (Train): 100%|██████████| 171/171 [00:06<00:00, 27.84it/s, acc=0.604, loss=0.667]\n",
      "Epoch 5 (Val): 100%|██████████| 22/22 [00:00<00:00, 43.41it/s, acc=0.515, loss=0.777]\n",
      "Epoch 6 (Train): 100%|██████████| 171/171 [00:06<00:00, 27.23it/s, acc=0.604, loss=0.668]\n",
      "Epoch 6 (Val): 100%|██████████| 22/22 [00:00<00:00, 47.69it/s, acc=0.515, loss=0.774]\n",
      "Epoch 7 (Train): 100%|██████████| 171/171 [00:06<00:00, 25.28it/s, acc=0.598, loss=0.669]\n",
      "Epoch 7 (Val): 100%|██████████| 22/22 [00:01<00:00, 21.21it/s, acc=0.515, loss=0.772]\n",
      "Epoch 8 (Train): 100%|██████████| 171/171 [00:16<00:00, 10.34it/s, acc=0.598, loss=0.669]\n",
      "Epoch 8 (Val): 100%|██████████| 22/22 [00:01<00:00, 18.21it/s, acc=0.515, loss=0.77] \n",
      "Epoch 9 (Train): 100%|██████████| 171/171 [00:06<00:00, 26.89it/s, acc=0.598, loss=0.67] \n",
      "Epoch 9 (Val): 100%|██████████| 22/22 [00:00<00:00, 47.68it/s, acc=0.515, loss=0.768]\n",
      "Epoch 10 (Train): 100%|██████████| 171/171 [00:07<00:00, 24.04it/s, acc=0.592, loss=0.671]\n",
      "Epoch 10 (Val): 100%|██████████| 22/22 [00:00<00:00, 45.01it/s, acc=0.515, loss=0.766]\n",
      "Epoch 11 (Train): 100%|██████████| 171/171 [00:06<00:00, 24.71it/s, acc=0.592, loss=0.671]\n",
      "Epoch 11 (Val): 100%|██████████| 22/22 [00:00<00:00, 43.72it/s, acc=0.515, loss=0.764]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- batch_size_50; lr_0.01; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Train): 100%|██████████| 171/171 [00:06<00:00, 26.19it/s, acc=0.785, loss=0.513]\n",
      "Epoch 1 (Val): 100%|██████████| 22/22 [00:00<00:00, 45.55it/s, acc=0.515, loss=0.997]\n",
      "Epoch 2 (Train): 100%|██████████| 171/171 [00:08<00:00, 20.77it/s, acc=0.674, loss=0.612]\n",
      "Epoch 2 (Val): 100%|██████████| 22/22 [00:00<00:00, 44.26it/s, acc=0.515, loss=0.997]\n",
      "Epoch 3 (Train): 100%|██████████| 171/171 [00:06<00:00, 26.20it/s, acc=0.663, loss=0.625]\n",
      "Epoch 3 (Val): 100%|██████████| 22/22 [00:00<00:00, 49.07it/s, acc=0.515, loss=0.975]\n",
      "Epoch 4 (Train): 100%|██████████| 171/171 [00:06<00:00, 25.84it/s, acc=0.657, loss=0.631]\n",
      "Epoch 4 (Val): 100%|██████████| 22/22 [00:00<00:00, 35.21it/s, acc=0.515, loss=0.956]\n",
      "Epoch 5 (Train): 100%|██████████| 171/171 [00:06<00:00, 25.43it/s, acc=0.651, loss=0.635]\n",
      "Epoch 5 (Val): 100%|██████████| 22/22 [00:00<00:00, 43.72it/s, acc=0.515, loss=0.94] \n",
      "Epoch 6 (Train): 100%|██████████| 171/171 [00:06<00:00, 25.78it/s, acc=0.645, loss=0.639]\n",
      "Epoch 6 (Val): 100%|██████████| 22/22 [00:00<00:00, 46.45it/s, acc=0.515, loss=0.925]\n",
      "Epoch 7 (Train): 100%|██████████| 171/171 [00:06<00:00, 26.73it/s, acc=0.639, loss=0.643]\n",
      "Epoch 7 (Val): 100%|██████████| 22/22 [00:00<00:00, 46.42it/s, acc=0.515, loss=0.911]\n",
      "Epoch 8 (Train): 100%|██████████| 171/171 [00:06<00:00, 28.24it/s, acc=0.639, loss=0.646]\n",
      "Epoch 8 (Val): 100%|██████████| 22/22 [00:00<00:00, 49.79it/s, acc=0.515, loss=0.898]\n",
      "Epoch 9 (Train): 100%|██████████| 171/171 [00:06<00:00, 27.51it/s, acc=0.627, loss=0.649]\n",
      "Epoch 9 (Val): 100%|██████████| 22/22 [00:00<00:00, 48.97it/s, acc=0.515, loss=0.886]\n",
      "Epoch 10 (Train): 100%|██████████| 171/171 [00:06<00:00, 25.17it/s, acc=0.627, loss=0.652]\n",
      "Epoch 10 (Val): 100%|██████████| 22/22 [00:00<00:00, 46.74it/s, acc=0.515, loss=0.874]\n",
      "Epoch 11 (Train): 100%|██████████| 171/171 [00:06<00:00, 26.43it/s, acc=0.622, loss=0.655]\n",
      "Epoch 11 (Val): 100%|██████████| 22/22 [00:00<00:00, 48.44it/s, acc=0.515, loss=0.863]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- batch_size_50; lr_0.025; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Train): 100%|██████████| 171/171 [00:06<00:00, 26.39it/s, acc=0.914, loss=0.279]\n",
      "Epoch 1 (Val): 100%|██████████| 22/22 [00:00<00:00, 45.02it/s, acc=0.515, loss=1.59] \n",
      "Epoch 2 (Train): 100%|██████████| 171/171 [00:07<00:00, 24.33it/s, acc=0.838, loss=0.403]\n",
      "Epoch 2 (Val): 100%|██████████| 22/22 [00:00<00:00, 47.71it/s, acc=0.515, loss=1.55] \n",
      "Epoch 3 (Train): 100%|██████████| 171/171 [00:06<00:00, 25.46it/s, acc=0.832, loss=0.409]\n",
      "Epoch 3 (Val): 100%|██████████| 22/22 [00:00<00:00, 45.42it/s, acc=0.515, loss=1.54] \n",
      "Epoch 4 (Train): 100%|██████████| 171/171 [00:09<00:00, 17.24it/s, acc=0.826, loss=0.416]\n",
      "Epoch 4 (Val): 100%|██████████| 22/22 [00:00<00:00, 46.44it/s, acc=0.515, loss=1.52] \n",
      "Epoch 5 (Train): 100%|██████████| 171/171 [00:07<00:00, 21.70it/s, acc=0.826, loss=0.423]\n",
      "Epoch 5 (Val): 100%|██████████| 22/22 [00:00<00:00, 46.03it/s, acc=0.515, loss=1.5]  \n",
      "Epoch 6 (Train): 100%|██████████| 171/171 [00:07<00:00, 24.04it/s, acc=0.815, loss=0.43] \n",
      "Epoch 6 (Val): 100%|██████████| 22/22 [00:00<00:00, 51.24it/s, acc=0.515, loss=1.48] \n",
      "Epoch 7 (Train): 100%|██████████| 171/171 [00:05<00:00, 29.14it/s, acc=0.809, loss=0.437]\n",
      "Epoch 7 (Val): 100%|██████████| 22/22 [00:00<00:00, 51.35it/s, acc=0.515, loss=1.47] \n",
      "Epoch 8 (Train): 100%|██████████| 171/171 [00:05<00:00, 29.22it/s, acc=0.809, loss=0.443]\n",
      "Epoch 8 (Val): 100%|██████████| 22/22 [00:00<00:00, 52.36it/s, acc=0.515, loss=1.47] \n",
      "Epoch 9 (Train): 100%|██████████| 171/171 [00:05<00:00, 29.62it/s, acc=0.809, loss=0.446]\n",
      "Epoch 9 (Val): 100%|██████████| 22/22 [00:00<00:00, 54.65it/s, acc=0.515, loss=1.47] \n",
      "Epoch 10 (Train): 100%|██████████| 171/171 [00:06<00:00, 27.49it/s, acc=0.803, loss=0.446]\n",
      "Epoch 10 (Val): 100%|██████████| 22/22 [00:00<00:00, 49.56it/s, acc=0.515, loss=1.46] \n",
      "Epoch 11 (Train): 100%|██████████| 171/171 [00:06<00:00, 28.21it/s, acc=0.797, loss=0.448]\n",
      "Epoch 11 (Val): 100%|██████████| 22/22 [00:00<00:00, 43.67it/s, acc=0.515, loss=1.45] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- batch_size_50; lr_0.05; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Train): 100%|██████████| 171/171 [00:08<00:00, 20.31it/s, acc=0.961, loss=0.148]\n",
      "Epoch 1 (Val): 100%|██████████| 22/22 [00:00<00:00, 36.48it/s, acc=0.515, loss=2.17] \n",
      "Epoch 2 (Train): 100%|██████████| 171/171 [00:06<00:00, 26.22it/s, acc=0.891, loss=0.295]\n",
      "Epoch 2 (Val): 100%|██████████| 22/22 [00:01<00:00, 20.78it/s, acc=0.515, loss=2.03] \n",
      "Epoch 3 (Train): 100%|██████████| 171/171 [00:07<00:00, 22.94it/s, acc=0.891, loss=0.291]\n",
      "Epoch 3 (Val): 100%|██████████| 22/22 [00:00<00:00, 35.22it/s, acc=0.515, loss=2]    \n",
      "Epoch 4 (Train): 100%|██████████| 171/171 [00:06<00:00, 25.23it/s, acc=0.896, loss=0.285]\n",
      "Epoch 4 (Val): 100%|██████████| 22/22 [00:00<00:00, 50.21it/s, acc=0.515, loss=2.04] \n",
      "Epoch 5 (Train): 100%|██████████| 171/171 [00:06<00:00, 28.17it/s, acc=0.896, loss=0.28] \n",
      "Epoch 5 (Val): 100%|██████████| 22/22 [00:00<00:00, 40.44it/s, acc=0.515, loss=2.04] \n",
      "Epoch 6 (Train): 100%|██████████| 171/171 [00:07<00:00, 22.76it/s, acc=0.896, loss=0.282]\n",
      "Epoch 6 (Val): 100%|██████████| 22/22 [00:01<00:00, 17.67it/s, acc=0.515, loss=2.03] \n",
      "Epoch 7 (Train): 100%|██████████| 171/171 [00:12<00:00, 14.17it/s, acc=0.891, loss=0.287]\n",
      "Epoch 7 (Val): 100%|██████████| 22/22 [00:00<00:00, 43.24it/s, acc=0.515, loss=2.01] \n",
      "Epoch 8 (Train): 100%|██████████| 171/171 [00:06<00:00, 26.38it/s, acc=0.891, loss=0.292]\n",
      "Epoch 8 (Val): 100%|██████████| 22/22 [00:00<00:00, 49.43it/s, acc=0.515, loss=2]    \n",
      "Epoch 9 (Train): 100%|██████████| 171/171 [00:05<00:00, 28.59it/s, acc=0.891, loss=0.295]\n",
      "Epoch 9 (Val): 100%|██████████| 22/22 [00:00<00:00, 45.58it/s, acc=0.515, loss=2]    \n",
      "Epoch 10 (Train): 100%|██████████| 171/171 [00:06<00:00, 25.61it/s, acc=0.885, loss=0.297]\n",
      "Epoch 10 (Val): 100%|██████████| 22/22 [00:00<00:00, 42.46it/s, acc=0.515, loss=2.01]\n",
      "Epoch 11 (Train): 100%|██████████| 171/171 [00:06<00:00, 26.63it/s, acc=0.885, loss=0.3]  \n",
      "Epoch 11 (Val): 100%|██████████| 22/22 [00:00<00:00, 48.87it/s, acc=0.515, loss=2]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- batch_size_100; lr_0.005; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Train): 100%|██████████| 86/86 [00:04<00:00, 18.32it/s, acc=0.566, loss=0.689] \n",
      "Epoch 1 (Val): 100%|██████████| 11/11 [00:00<00:00, 28.92it/s, acc=0.515, loss=0.726]\n",
      "Epoch 2 (Train): 100%|██████████| 86/86 [00:04<00:00, 19.67it/s, acc=0.554, loss=0.688] \n",
      "Epoch 2 (Val): 100%|██████████| 11/11 [00:00<00:00, 29.71it/s, acc=0.515, loss=0.724]\n",
      "Epoch 3 (Train): 100%|██████████| 86/86 [00:04<00:00, 18.56it/s, acc=0.554, loss=0.688] \n",
      "Epoch 3 (Val): 100%|██████████| 11/11 [00:00<00:00, 29.92it/s, acc=0.515, loss=0.722]\n",
      "Epoch 4 (Train): 100%|██████████| 86/86 [00:04<00:00, 18.62it/s, acc=0.554, loss=0.688] \n",
      "Epoch 4 (Val): 100%|██████████| 11/11 [00:00<00:00, 29.78it/s, acc=0.515, loss=0.721]\n",
      "Epoch 5 (Train): 100%|██████████| 86/86 [00:04<00:00, 19.73it/s, acc=0.554, loss=0.688] \n",
      "Epoch 5 (Val): 100%|██████████| 11/11 [00:00<00:00, 30.80it/s, acc=0.515, loss=0.719]\n",
      "Epoch 6 (Train): 100%|██████████| 86/86 [00:04<00:00, 20.50it/s, acc=0.554, loss=0.689] \n",
      "Epoch 6 (Val): 100%|██████████| 11/11 [00:00<00:00, 31.90it/s, acc=0.515, loss=0.718]\n",
      "Epoch 7 (Train): 100%|██████████| 86/86 [00:04<00:00, 21.00it/s, acc=0.554, loss=0.689] \n",
      "Epoch 7 (Val): 100%|██████████| 11/11 [00:00<00:00, 29.49it/s, acc=0.515, loss=0.717]\n",
      "Epoch 8 (Train): 100%|██████████| 86/86 [00:04<00:00, 20.24it/s, acc=0.542, loss=0.689] \n",
      "Epoch 8 (Val): 100%|██████████| 11/11 [00:00<00:00, 30.77it/s, acc=0.515, loss=0.716]\n",
      "Epoch 9 (Train): 100%|██████████| 86/86 [00:04<00:00, 18.75it/s, acc=0.542, loss=0.689] \n",
      "Epoch 9 (Val): 100%|██████████| 11/11 [00:00<00:00, 29.37it/s, acc=0.515, loss=0.715]\n",
      "Epoch 10 (Train): 100%|██████████| 86/86 [00:04<00:00, 19.39it/s, acc=0.542, loss=0.69]  \n",
      "Epoch 10 (Val): 100%|██████████| 11/11 [00:00<00:00, 24.85it/s, acc=0.515, loss=0.714]\n",
      "Epoch 11 (Train): 100%|██████████| 86/86 [00:04<00:00, 19.10it/s, acc=0.542, loss=0.69]  \n",
      "Epoch 11 (Val): 100%|██████████| 11/11 [00:00<00:00, 29.21it/s, acc=0.515, loss=0.714]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- batch_size_100; lr_0.01; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Train): 100%|██████████| 86/86 [00:04<00:00, 20.19it/s, acc=0.659, loss=0.655]\n",
      "Epoch 1 (Val): 100%|██████████| 11/11 [00:00<00:00, 33.03it/s, acc=0.515, loss=0.706]\n",
      "Epoch 2 (Train): 100%|██████████| 86/86 [00:04<00:00, 19.39it/s, acc=0.589, loss=0.672] \n",
      "Epoch 2 (Val): 100%|██████████| 11/11 [00:00<00:00, 27.18it/s, acc=0.515, loss=0.721]\n",
      "Epoch 3 (Train): 100%|██████████| 86/86 [00:04<00:00, 18.93it/s, acc=0.566, loss=0.682] \n",
      "Epoch 3 (Val): 100%|██████████| 11/11 [00:01<00:00, 10.28it/s, acc=0.515, loss=0.726]\n",
      "Epoch 4 (Train): 100%|██████████| 86/86 [00:12<00:00,  7.05it/s, acc=0.566, loss=0.686] \n",
      "Epoch 4 (Val): 100%|██████████| 11/11 [00:01<00:00, 10.71it/s, acc=0.515, loss=0.727]\n",
      "Epoch 5 (Train): 100%|██████████| 86/86 [00:11<00:00,  7.42it/s, acc=0.554, loss=0.687] \n",
      "Epoch 5 (Val): 100%|██████████| 11/11 [00:00<00:00, 12.27it/s, acc=0.515, loss=0.726]\n",
      "Epoch 6 (Train): 100%|██████████| 86/86 [00:05<00:00, 15.11it/s, acc=0.554, loss=0.688] \n",
      "Epoch 6 (Val): 100%|██████████| 11/11 [00:00<00:00, 20.00it/s, acc=0.515, loss=0.725]\n",
      "Epoch 7 (Train): 100%|██████████| 86/86 [00:05<00:00, 15.57it/s, acc=0.554, loss=0.688] \n",
      "Epoch 7 (Val): 100%|██████████| 11/11 [00:00<00:00, 23.84it/s, acc=0.515, loss=0.724]\n",
      "Epoch 8 (Train): 100%|██████████| 86/86 [00:05<00:00, 16.46it/s, acc=0.554, loss=0.689] \n",
      "Epoch 8 (Val): 100%|██████████| 11/11 [00:00<00:00, 26.12it/s, acc=0.515, loss=0.722]\n",
      "Epoch 9 (Train): 100%|██████████| 86/86 [00:04<00:00, 17.85it/s, acc=0.554, loss=0.689] \n",
      "Epoch 9 (Val): 100%|██████████| 11/11 [00:00<00:00, 27.31it/s, acc=0.515, loss=0.721]\n",
      "Epoch 10 (Train): 100%|██████████| 86/86 [00:05<00:00, 17.01it/s, acc=0.554, loss=0.689] \n",
      "Epoch 10 (Val): 100%|██████████| 11/11 [00:00<00:00, 23.60it/s, acc=0.515, loss=0.72]\n",
      "Epoch 11 (Train): 100%|██████████| 86/86 [00:04<00:00, 18.57it/s, acc=0.554, loss=0.689] \n",
      "Epoch 11 (Val): 100%|██████████| 11/11 [00:00<00:00, 32.02it/s, acc=0.515, loss=0.719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- batch_size_100; lr_0.025; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Train): 100%|██████████| 86/86 [00:04<00:00, 19.47it/s, acc=0.787, loss=0.502]\n",
      "Epoch 1 (Val): 100%|██████████| 11/11 [00:00<00:00, 29.96it/s, acc=0.515, loss=1.02]\n",
      "Epoch 2 (Train): 100%|██████████| 86/86 [00:04<00:00, 19.04it/s, acc=0.67, loss=0.621] \n",
      "Epoch 2 (Val): 100%|██████████| 11/11 [00:00<00:00, 23.54it/s, acc=0.515, loss=0.998]\n",
      "Epoch 3 (Train): 100%|██████████| 86/86 [00:09<00:00,  8.62it/s, acc=0.659, loss=0.636]\n",
      "Epoch 3 (Val): 100%|██████████| 11/11 [00:00<00:00, 13.80it/s, acc=0.515, loss=0.958]\n",
      "Epoch 4 (Train): 100%|██████████| 86/86 [00:09<00:00,  8.68it/s, acc=0.647, loss=0.645]\n",
      "Epoch 4 (Val): 100%|██████████| 11/11 [00:00<00:00, 12.67it/s, acc=0.515, loss=0.925]\n",
      "Epoch 5 (Train): 100%|██████████| 86/86 [00:11<00:00,  7.40it/s, acc=0.635, loss=0.652]\n",
      "Epoch 5 (Val): 100%|██████████| 11/11 [00:00<00:00, 11.48it/s, acc=0.515, loss=0.896]\n",
      "Epoch 6 (Train): 100%|██████████| 86/86 [00:12<00:00,  7.03it/s, acc=0.624, loss=0.657]\n",
      "Epoch 6 (Val): 100%|██████████| 11/11 [00:00<00:00, 11.47it/s, acc=0.515, loss=0.872]\n",
      "Epoch 7 (Train): 100%|██████████| 86/86 [00:10<00:00,  8.06it/s, acc=0.624, loss=0.662] \n",
      "Epoch 7 (Val): 100%|██████████| 11/11 [00:00<00:00, 24.87it/s, acc=0.515, loss=0.851]\n",
      "Epoch 8 (Train): 100%|██████████| 86/86 [00:04<00:00, 18.56it/s, acc=0.612, loss=0.665]\n",
      "Epoch 8 (Val): 100%|██████████| 11/11 [00:00<00:00, 26.27it/s, acc=0.515, loss=0.833]\n",
      "Epoch 9 (Train): 100%|██████████| 86/86 [00:04<00:00, 19.57it/s, acc=0.612, loss=0.668] \n",
      "Epoch 9 (Val): 100%|██████████| 11/11 [00:00<00:00, 31.49it/s, acc=0.515, loss=0.818]\n",
      "Epoch 10 (Train): 100%|██████████| 86/86 [00:04<00:00, 18.49it/s, acc=0.601, loss=0.671] \n",
      "Epoch 10 (Val): 100%|██████████| 11/11 [00:00<00:00, 29.67it/s, acc=0.515, loss=0.805]\n",
      "Epoch 11 (Train): 100%|██████████| 86/86 [00:04<00:00, 17.58it/s, acc=0.589, loss=0.673]\n",
      "Epoch 11 (Val): 100%|██████████| 11/11 [00:00<00:00, 25.91it/s, acc=0.515, loss=0.795]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- batch_size_100; lr_0.05; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Train): 100%|██████████| 86/86 [00:04<00:00, 19.21it/s, acc=0.926, loss=0.256]\n",
      "Epoch 1 (Val): 100%|██████████| 11/11 [00:00<00:00, 27.73it/s, acc=0.515, loss=1.71]\n",
      "Epoch 2 (Train): 100%|██████████| 86/86 [00:04<00:00, 19.87it/s, acc=0.856, loss=0.376]\n",
      "Epoch 2 (Val): 100%|██████████| 11/11 [00:00<00:00, 30.19it/s, acc=0.515, loss=1.67]\n",
      "Epoch 3 (Train): 100%|██████████| 86/86 [00:04<00:00, 20.61it/s, acc=0.845, loss=0.387]\n",
      "Epoch 3 (Val): 100%|██████████| 11/11 [00:00<00:00, 29.88it/s, acc=0.515, loss=1.65]\n",
      "Epoch 4 (Train): 100%|██████████| 86/86 [00:04<00:00, 20.08it/s, acc=0.845, loss=0.396]\n",
      "Epoch 4 (Val): 100%|██████████| 11/11 [00:00<00:00, 26.17it/s, acc=0.515, loss=1.63]\n",
      "Epoch 5 (Train): 100%|██████████| 86/86 [00:04<00:00, 19.08it/s, acc=0.833, loss=0.404]\n",
      "Epoch 5 (Val): 100%|██████████| 11/11 [00:00<00:00, 29.90it/s, acc=0.515, loss=1.61]\n",
      "Epoch 6 (Train): 100%|██████████| 86/86 [00:12<00:00,  7.07it/s, acc=0.833, loss=0.413]\n",
      "Epoch 6 (Val): 100%|██████████| 11/11 [00:01<00:00, 10.14it/s, acc=0.515, loss=1.6]\n",
      "Epoch 7 (Train): 100%|██████████| 86/86 [00:11<00:00,  7.21it/s, acc=0.822, loss=0.421]\n",
      "Epoch 7 (Val): 100%|██████████| 11/11 [00:01<00:00, 10.49it/s, acc=0.515, loss=1.58]\n",
      "Epoch 8 (Train): 100%|██████████| 86/86 [00:12<00:00,  7.03it/s, acc=0.822, loss=0.43] \n",
      "Epoch 8 (Val): 100%|██████████| 11/11 [00:01<00:00,  8.25it/s, acc=0.515, loss=1.56]\n",
      "Epoch 9 (Train): 100%|██████████| 86/86 [00:05<00:00, 15.66it/s, acc=0.822, loss=0.438]\n",
      "Epoch 9 (Val): 100%|██████████| 11/11 [00:00<00:00, 29.12it/s, acc=0.515, loss=1.55]\n",
      "Epoch 10 (Train): 100%|██████████| 86/86 [00:04<00:00, 18.33it/s, acc=0.81, loss=0.447] \n",
      "Epoch 10 (Val): 100%|██████████| 11/11 [00:00<00:00, 26.95it/s, acc=0.515, loss=1.53]\n",
      "Epoch 11 (Train): 100%|██████████| 86/86 [00:04<00:00, 18.53it/s, acc=0.798, loss=0.456]\n",
      "Epoch 11 (Val): 100%|██████████| 11/11 [00:00<00:00, 28.18it/s, acc=0.515, loss=1.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- batch_size_200; lr_0.005; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Train): 100%|██████████| 43/43 [00:04<00:00, 10.20it/s, acc=0.566, loss=0.688]\n",
      "Epoch 1 (Val): 100%|██████████| 6/6 [00:00<00:00, 15.26it/s, acc=0.556, loss=0.687]\n",
      "Epoch 2 (Train): 100%|██████████| 43/43 [00:03<00:00, 11.24it/s, acc=0.542, loss=0.691]\n",
      "Epoch 2 (Val): 100%|██████████| 6/6 [00:00<00:00, 16.69it/s, acc=0.556, loss=0.687]\n",
      "Epoch 3 (Train): 100%|██████████| 43/43 [00:03<00:00, 10.92it/s, acc=0.519, loss=0.693]\n",
      "Epoch 3 (Val): 100%|██████████| 6/6 [00:00<00:00, 15.36it/s, acc=0.556, loss=0.687]\n",
      "Epoch 4 (Train): 100%|██████████| 43/43 [00:03<00:00, 11.45it/s, acc=0.519, loss=0.694] \n",
      "Epoch 4 (Val): 100%|██████████| 6/6 [00:00<00:00, 17.99it/s, acc=0.556, loss=0.687]\n",
      "Epoch 5 (Train): 100%|██████████| 43/43 [00:03<00:00, 12.46it/s, acc=0.519, loss=0.695] \n",
      "Epoch 5 (Val): 100%|██████████| 6/6 [00:00<00:00, 15.44it/s, acc=0.556, loss=0.687]\n",
      "Epoch 6 (Train): 100%|██████████| 43/43 [00:03<00:00, 12.32it/s, acc=0.496, loss=0.696]\n",
      "Epoch 6 (Val): 100%|██████████| 6/6 [00:00<00:00, 17.57it/s, acc=0.556, loss=0.687]\n",
      "Epoch 7 (Train): 100%|██████████| 43/43 [00:03<00:00, 11.90it/s, acc=0.519, loss=0.696] \n",
      "Epoch 7 (Val): 100%|██████████| 6/6 [00:00<00:00, 16.96it/s, acc=0.556, loss=0.687]\n",
      "Epoch 8 (Train): 100%|██████████| 43/43 [00:03<00:00, 11.58it/s, acc=0.519, loss=0.696] \n",
      "Epoch 8 (Val): 100%|██████████| 6/6 [00:00<00:00, 17.15it/s, acc=0.556, loss=0.687]\n",
      "Epoch 9 (Train): 100%|██████████| 43/43 [00:06<00:00,  7.09it/s, acc=0.519, loss=0.696] \n",
      "Epoch 9 (Val): 100%|██████████| 6/6 [00:00<00:00,  6.19it/s, acc=0.556, loss=0.687]\n",
      "Epoch 10 (Train): 100%|██████████| 43/43 [00:03<00:00, 11.19it/s, acc=0.519, loss=0.696] \n",
      "Epoch 10 (Val): 100%|██████████| 6/6 [00:00<00:00, 17.79it/s, acc=0.556, loss=0.687]\n",
      "Epoch 11 (Train): 100%|██████████| 43/43 [00:04<00:00, 10.18it/s, acc=0.519, loss=0.696] \n",
      "Epoch 11 (Val): 100%|██████████| 6/6 [00:00<00:00, 15.40it/s, acc=0.556, loss=0.687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- batch_size_200; lr_0.01; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Train): 100%|██████████| 43/43 [00:08<00:00,  5.23it/s, acc=0.519, loss=0.695] \n",
      "Epoch 1 (Val): 100%|██████████| 6/6 [00:00<00:00,  6.11it/s, acc=0.556, loss=0.694]\n",
      "Epoch 2 (Train): 100%|██████████| 43/43 [00:09<00:00,  4.38it/s, acc=0.519, loss=0.696] \n",
      "Epoch 2 (Val): 100%|██████████| 6/6 [00:01<00:00,  5.77it/s, acc=0.556, loss=0.693]\n",
      "Epoch 3 (Train): 100%|██████████| 43/43 [00:09<00:00,  4.47it/s, acc=0.519, loss=0.696] \n",
      "Epoch 3 (Val): 100%|██████████| 6/6 [00:00<00:00,  6.17it/s, acc=0.556, loss=0.693]\n",
      "Epoch 4 (Train): 100%|██████████| 43/43 [00:09<00:00,  4.45it/s, acc=0.519, loss=0.696] \n",
      "Epoch 4 (Val): 100%|██████████| 6/6 [00:00<00:00,  6.11it/s, acc=0.556, loss=0.692]\n",
      "Epoch 5 (Train): 100%|██████████| 43/43 [00:05<00:00,  7.88it/s, acc=0.519, loss=0.696] \n",
      "Epoch 5 (Val): 100%|██████████| 6/6 [00:00<00:00, 15.66it/s, acc=0.556, loss=0.692]\n",
      "Epoch 6 (Train): 100%|██████████| 43/43 [00:03<00:00, 11.18it/s, acc=0.519, loss=0.696] \n",
      "Epoch 6 (Val): 100%|██████████| 6/6 [00:00<00:00, 18.15it/s, acc=0.556, loss=0.691]\n",
      "Epoch 7 (Train): 100%|██████████| 43/43 [00:06<00:00,  6.39it/s, acc=0.519, loss=0.696] \n",
      "Epoch 7 (Val): 100%|██████████| 6/6 [00:01<00:00,  5.69it/s, acc=0.556, loss=0.691]\n",
      "Epoch 8 (Train): 100%|██████████| 43/43 [00:08<00:00,  5.06it/s, acc=0.519, loss=0.696] \n",
      "Epoch 8 (Val): 100%|██████████| 6/6 [00:00<00:00,  7.00it/s, acc=0.556, loss=0.691]\n",
      "Epoch 9 (Train): 100%|██████████| 43/43 [00:09<00:00,  4.66it/s, acc=0.519, loss=0.696] \n",
      "Epoch 9 (Val): 100%|██████████| 6/6 [00:01<00:00,  5.61it/s, acc=0.556, loss=0.691]\n",
      "Epoch 10 (Train): 100%|██████████| 43/43 [00:07<00:00,  5.80it/s, acc=0.519, loss=0.696] \n",
      "Epoch 10 (Val): 100%|██████████| 6/6 [00:00<00:00, 15.62it/s, acc=0.556, loss=0.69] \n",
      "Epoch 11 (Train): 100%|██████████| 43/43 [00:03<00:00, 11.64it/s, acc=0.519, loss=0.696] \n",
      "Epoch 11 (Val): 100%|██████████| 6/6 [00:00<00:00, 15.73it/s, acc=0.556, loss=0.69] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- batch_size_200; lr_0.025; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Train): 100%|██████████| 43/43 [00:03<00:00, 10.81it/s, acc=0.728, loss=0.58] \n",
      "Epoch 1 (Val): 100%|██████████| 6/6 [00:00<00:00, 17.04it/s, acc=0.556, loss=0.793]\n",
      "Epoch 2 (Train): 100%|██████████| 43/43 [00:03<00:00, 11.77it/s, acc=0.612, loss=0.68] \n",
      "Epoch 2 (Val): 100%|██████████| 6/6 [00:00<00:00, 18.16it/s, acc=0.556, loss=0.76] \n",
      "Epoch 3 (Train): 100%|██████████| 43/43 [00:03<00:00, 11.09it/s, acc=0.589, loss=0.691]\n",
      "Epoch 3 (Val): 100%|██████████| 6/6 [00:00<00:00, 17.07it/s, acc=0.556, loss=0.735]\n",
      "Epoch 4 (Train): 100%|██████████| 43/43 [00:03<00:00, 12.27it/s, acc=0.566, loss=0.693] \n",
      "Epoch 4 (Val): 100%|██████████| 6/6 [00:00<00:00, 16.09it/s, acc=0.556, loss=0.721]\n",
      "Epoch 5 (Train): 100%|██████████| 43/43 [00:03<00:00, 11.61it/s, acc=0.566, loss=0.694]\n",
      "Epoch 5 (Val): 100%|██████████| 6/6 [00:00<00:00, 16.00it/s, acc=0.556, loss=0.712]\n",
      "Epoch 6 (Train): 100%|██████████| 43/43 [00:03<00:00, 10.80it/s, acc=0.542, loss=0.694] \n",
      "Epoch 6 (Val): 100%|██████████| 6/6 [00:00<00:00, 16.37it/s, acc=0.556, loss=0.707]\n",
      "Epoch 7 (Train): 100%|██████████| 43/43 [00:03<00:00, 12.00it/s, acc=0.542, loss=0.695] \n",
      "Epoch 7 (Val): 100%|██████████| 6/6 [00:00<00:00, 14.92it/s, acc=0.556, loss=0.703]\n",
      "Epoch 8 (Train): 100%|██████████| 43/43 [00:06<00:00,  6.18it/s, acc=0.542, loss=0.695] \n",
      "Epoch 8 (Val): 100%|██████████| 6/6 [00:01<00:00,  5.99it/s, acc=0.556, loss=0.701]\n",
      "Epoch 9 (Train): 100%|██████████| 43/43 [00:09<00:00,  4.47it/s, acc=0.542, loss=0.695] \n",
      "Epoch 9 (Val): 100%|██████████| 6/6 [00:01<00:00,  5.91it/s, acc=0.556, loss=0.699]\n",
      "Epoch 10 (Train): 100%|██████████| 43/43 [00:09<00:00,  4.43it/s, acc=0.542, loss=0.696] \n",
      "Epoch 10 (Val): 100%|██████████| 6/6 [00:01<00:00,  5.90it/s, acc=0.556, loss=0.697]\n",
      "Epoch 11 (Train): 100%|██████████| 43/43 [00:09<00:00,  4.43it/s, acc=0.542, loss=0.696] \n",
      "Epoch 11 (Val): 100%|██████████| 6/6 [00:00<00:00,  6.27it/s, acc=0.556, loss=0.696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- batch_size_200; lr_0.05; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Train): 100%|██████████| 43/43 [00:10<00:00,  4.20it/s, acc=0.775, loss=0.485]\n",
      "Epoch 1 (Val): 100%|██████████| 6/6 [00:01<00:00,  5.67it/s, acc=0.556, loss=1.08]\n",
      "Epoch 2 (Train): 100%|██████████| 43/43 [00:10<00:00,  4.25it/s, acc=0.705, loss=0.599]\n",
      "Epoch 2 (Val): 100%|██████████| 6/6 [00:01<00:00,  5.30it/s, acc=0.556, loss=1.06]\n",
      "Epoch 3 (Train): 100%|██████████| 43/43 [00:08<00:00,  4.78it/s, acc=0.705, loss=0.61] \n",
      "Epoch 3 (Val): 100%|██████████| 6/6 [00:01<00:00,  5.71it/s, acc=0.556, loss=1.01]\n",
      "Epoch 4 (Train): 100%|██████████| 43/43 [00:10<00:00,  4.26it/s, acc=0.682, loss=0.625]\n",
      "Epoch 4 (Val): 100%|██████████| 6/6 [00:01<00:00,  5.30it/s, acc=0.556, loss=0.963]\n",
      "Epoch 5 (Train): 100%|██████████| 43/43 [00:09<00:00,  4.60it/s, acc=0.682, loss=0.64] \n",
      "Epoch 5 (Val): 100%|██████████| 6/6 [00:00<00:00,  7.00it/s, acc=0.556, loss=0.914]\n",
      "Epoch 6 (Train): 100%|██████████| 43/43 [00:08<00:00,  4.91it/s, acc=0.635, loss=0.653]\n",
      "Epoch 6 (Val): 100%|██████████| 6/6 [00:00<00:00,  6.59it/s, acc=0.556, loss=0.868]\n",
      "Epoch 7 (Train): 100%|██████████| 43/43 [00:09<00:00,  4.46it/s, acc=0.635, loss=0.666]\n",
      "Epoch 7 (Val): 100%|██████████| 6/6 [00:00<00:00,  6.00it/s, acc=0.556, loss=0.824]\n",
      "Epoch 8 (Train): 100%|██████████| 43/43 [00:09<00:00,  4.58it/s, acc=0.612, loss=0.677]\n",
      "Epoch 8 (Val): 100%|██████████| 6/6 [00:00<00:00,  6.16it/s, acc=0.556, loss=0.785]\n",
      "Epoch 9 (Train): 100%|██████████| 43/43 [00:08<00:00,  4.87it/s, acc=0.589, loss=0.685]\n",
      "Epoch 9 (Val): 100%|██████████| 6/6 [00:01<00:00,  5.73it/s, acc=0.556, loss=0.752]\n",
      "Epoch 10 (Train): 100%|██████████| 43/43 [00:09<00:00,  4.39it/s, acc=0.566, loss=0.69]  \n",
      "Epoch 10 (Val): 100%|██████████| 6/6 [00:00<00:00,  6.13it/s, acc=0.556, loss=0.729]\n",
      "Epoch 11 (Train): 100%|██████████| 43/43 [00:09<00:00,  4.58it/s, acc=0.566, loss=0.692] \n",
      "Epoch 11 (Val): 100%|██████████| 6/6 [00:01<00:00,  5.09it/s, acc=0.556, loss=0.716]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- batch_size_50; lr_0.005; optimizer_Adagrad; hidden_dim_16; num_layers_2 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Train): 100%|██████████| 171/171 [00:16<00:00, 10.10it/s, acc=0.908, loss=0.327]\n",
      "Epoch 1 (Val): 100%|██████████| 22/22 [00:01<00:00, 18.08it/s, acc=0.515, loss=1.18] \n",
      "Epoch 2 (Train): 100%|██████████| 171/171 [00:16<00:00, 10.14it/s, acc=0.75, loss=0.564] \n",
      "Epoch 2 (Val): 100%|██████████| 22/22 [00:00<00:00, 30.74it/s, acc=0.515, loss=0.987]\n",
      "Epoch 3 (Train): 100%|██████████| 171/171 [00:15<00:00, 10.84it/s, acc=0.686, loss=0.621]\n",
      "Epoch 3 (Val): 100%|██████████| 22/22 [00:01<00:00, 18.58it/s, acc=0.515, loss=0.887]\n",
      "Epoch 4 (Train): 100%|██████████| 171/171 [00:16<00:00, 10.13it/s, acc=0.645, loss=0.65] \n",
      "Epoch 4 (Val): 100%|██████████| 22/22 [00:01<00:00, 17.75it/s, acc=0.515, loss=0.829]\n",
      "Epoch 5 (Train): 100%|██████████| 171/171 [00:14<00:00, 11.94it/s, acc=0.622, loss=0.664]\n",
      "Epoch 5 (Val): 100%|██████████| 22/22 [00:00<00:00, 41.64it/s, acc=0.515, loss=0.793]\n",
      "Epoch 6 (Train): 100%|██████████| 171/171 [00:12<00:00, 13.67it/s, acc=0.604, loss=0.672]\n",
      "Epoch 6 (Val): 100%|██████████| 22/22 [00:00<00:00, 27.81it/s, acc=0.515, loss=0.771]\n",
      "Epoch 7 (Train): 100%|██████████| 171/171 [00:15<00:00, 10.86it/s, acc=0.592, loss=0.677]\n",
      "Epoch 7 (Val): 100%|██████████| 22/22 [00:01<00:00, 18.16it/s, acc=0.515, loss=0.755]\n",
      "Epoch 8 (Train): 100%|██████████| 171/171 [00:16<00:00, 10.30it/s, acc=0.581, loss=0.68] \n",
      "Epoch 8 (Val): 100%|██████████| 22/22 [00:01<00:00, 16.98it/s, acc=0.515, loss=0.744]\n",
      "Epoch 9 (Train): 100%|██████████| 171/171 [00:17<00:00, 10.02it/s, acc=0.575, loss=0.683]\n",
      "Epoch 9 (Val): 100%|██████████| 22/22 [00:01<00:00, 19.53it/s, acc=0.515, loss=0.735]\n",
      "Epoch 10 (Train): 100%|██████████| 171/171 [00:16<00:00, 10.61it/s, acc=0.563, loss=0.684]\n",
      "Epoch 10 (Val): 100%|██████████| 22/22 [00:01<00:00, 17.12it/s, acc=0.515, loss=0.729]\n",
      "Epoch 11 (Train): 100%|██████████| 171/171 [00:16<00:00, 10.24it/s, acc=0.563, loss=0.686]\n",
      "Epoch 11 (Val): 100%|██████████| 22/22 [00:01<00:00, 18.69it/s, acc=0.515, loss=0.724]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- batch_size_50; lr_0.01; optimizer_Adagrad; hidden_dim_16; num_layers_2 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Train): 100%|██████████| 171/171 [00:17<00:00,  9.56it/s, acc=0.967, loss=0.145]\n",
      "Epoch 1 (Val): 100%|██████████| 22/22 [00:01<00:00, 17.46it/s, acc=0.515, loss=1.97] \n",
      "Epoch 2 (Train): 100%|██████████| 171/171 [00:17<00:00, 10.01it/s, acc=0.908, loss=0.289]\n",
      "Epoch 2 (Val): 100%|██████████| 22/22 [00:00<00:00, 63.58it/s, acc=0.515, loss=1.64] \n",
      "Epoch 3 (Train): 100%|██████████| 171/171 [00:16<00:00, 10.44it/s, acc=0.861, loss=0.377]\n",
      "Epoch 3 (Val): 100%|██████████| 22/22 [00:01<00:00, 18.40it/s, acc=0.515, loss=1.47] \n",
      "Epoch 4 (Train): 100%|██████████| 171/171 [00:16<00:00, 10.32it/s, acc=0.82, loss=0.443] \n",
      "Epoch 4 (Val): 100%|██████████| 22/22 [00:01<00:00, 17.21it/s, acc=0.515, loss=1.34] \n",
      "Epoch 5 (Train): 100%|██████████| 171/171 [00:17<00:00,  9.93it/s, acc=0.785, loss=0.495]\n",
      "Epoch 5 (Val): 100%|██████████| 22/22 [00:01<00:00, 19.90it/s, acc=0.515, loss=1.23] \n",
      "Epoch 6 (Train): 100%|██████████| 171/171 [00:15<00:00, 11.11it/s, acc=0.756, loss=0.536]\n",
      "Epoch 6 (Val): 100%|██████████| 22/22 [00:01<00:00, 15.39it/s, acc=0.515, loss=1.14] \n",
      "Epoch 7 (Train): 100%|██████████| 171/171 [00:17<00:00,  9.72it/s, acc=0.727, loss=0.568]\n",
      "Epoch 7 (Val): 100%|██████████| 22/22 [00:01<00:00, 18.52it/s, acc=0.515, loss=1.07]\n",
      "Epoch 8 (Train): 100%|██████████| 171/171 [00:16<00:00, 10.18it/s, acc=0.704, loss=0.593]\n",
      "Epoch 8 (Val): 100%|██████████| 22/22 [00:01<00:00, 18.80it/s, acc=0.515, loss=1]    \n",
      "Epoch 9 (Train): 100%|██████████| 171/171 [00:15<00:00, 11.22it/s, acc=0.686, loss=0.612]\n",
      "Epoch 9 (Val): 100%|██████████| 22/22 [00:01<00:00, 17.31it/s, acc=0.515, loss=0.952]\n",
      "Epoch 10 (Train): 100%|██████████| 171/171 [00:17<00:00, 10.01it/s, acc=0.668, loss=0.627]\n",
      "Epoch 10 (Val): 100%|██████████| 22/22 [00:01<00:00, 18.69it/s, acc=0.515, loss=0.911]\n",
      "Epoch 11 (Train): 100%|██████████| 171/171 [00:16<00:00, 10.40it/s, acc=0.651, loss=0.638]\n",
      "Epoch 11 (Val): 100%|██████████| 22/22 [00:01<00:00, 19.10it/s, acc=0.515, loss=0.877]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- batch_size_50; lr_0.025; optimizer_Adagrad; hidden_dim_16; num_layers_2 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Train): 100%|██████████| 171/171 [00:17<00:00,  9.93it/s, acc=0.973, loss=0.106]\n",
      "Epoch 1 (Val): 100%|██████████| 22/22 [00:01<00:00, 14.92it/s, acc=0.515, loss=2.21] \n",
      "Epoch 2 (Train): 100%|██████████| 171/171 [00:17<00:00,  9.97it/s, acc=0.92, loss=0.266] \n",
      "Epoch 2 (Val): 100%|██████████| 22/22 [00:01<00:00, 16.59it/s, acc=0.515, loss=1.8]  \n",
      "Epoch 3 (Train): 100%|██████████| 171/171 [00:17<00:00,  9.89it/s, acc=0.873, loss=0.361]\n",
      "Epoch 3 (Val): 100%|██████████| 22/22 [00:01<00:00, 18.22it/s, acc=0.515, loss=1.57] \n",
      "Epoch 4 (Train): 100%|██████████| 171/171 [00:16<00:00, 10.46it/s, acc=0.826, loss=0.438]\n",
      "Epoch 4 (Val): 100%|██████████| 22/22 [00:01<00:00, 18.21it/s, acc=0.515, loss=1.37] \n",
      "Epoch 5 (Train): 100%|██████████| 171/171 [00:17<00:00,  9.92it/s, acc=0.774, loss=0.51] \n",
      "Epoch 5 (Val): 100%|██████████| 22/22 [00:01<00:00, 19.23it/s, acc=0.515, loss=1.24] \n",
      "Epoch 6 (Train): 100%|██████████| 171/171 [00:17<00:00,  9.98it/s, acc=0.733, loss=0.557]\n",
      "Epoch 6 (Val): 100%|██████████| 22/22 [00:01<00:00, 18.36it/s, acc=0.515, loss=1.14] \n",
      "Epoch 7 (Train): 100%|██████████| 171/171 [00:16<00:00, 10.30it/s, acc=0.709, loss=0.585]\n",
      "Epoch 7 (Val): 100%|██████████| 22/22 [00:01<00:00, 17.74it/s, acc=0.515, loss=1.05] \n",
      "Epoch 8 (Train): 100%|██████████| 171/171 [00:17<00:00,  9.92it/s, acc=0.686, loss=0.606]\n",
      "Epoch 8 (Val): 100%|██████████| 22/22 [00:01<00:00, 16.76it/s, acc=0.515, loss=0.988]\n",
      "Epoch 9 (Train): 100%|██████████| 171/171 [00:17<00:00,  9.63it/s, acc=0.668, loss=0.623]\n",
      "Epoch 9 (Val): 100%|██████████| 22/22 [00:01<00:00, 16.78it/s, acc=0.515, loss=0.937]\n",
      "Epoch 10 (Train): 100%|██████████| 171/171 [00:17<00:00,  9.77it/s, acc=0.657, loss=0.637]\n",
      "Epoch 10 (Val): 100%|██████████| 22/22 [00:01<00:00, 17.24it/s, acc=0.515, loss=0.895]\n",
      "Epoch 11 (Train): 100%|██████████| 171/171 [00:17<00:00,  9.88it/s, acc=0.633, loss=0.648]\n",
      "Epoch 11 (Val): 100%|██████████| 22/22 [00:01<00:00, 17.08it/s, acc=0.515, loss=0.859]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- batch_size_50; lr_0.05; optimizer_Adagrad; hidden_dim_16; num_layers_2 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Train): 100%|██████████| 171/171 [00:19<00:00,  8.77it/s, acc=0.978, loss=0.0854]\n",
      "Epoch 1 (Val): 100%|██████████| 22/22 [00:01<00:00, 16.97it/s, acc=0.515, loss=2.52] \n",
      "Epoch 2 (Train): 100%|██████████| 171/171 [00:17<00:00,  9.71it/s, acc=0.943, loss=0.206]\n",
      "Epoch 2 (Val): 100%|██████████| 22/22 [00:01<00:00, 17.88it/s, acc=0.515, loss=2.07] \n",
      "Epoch 3 (Train): 100%|██████████| 171/171 [00:18<00:00,  9.44it/s, acc=0.908, loss=0.28] \n",
      "Epoch 3 (Val): 100%|██████████| 22/22 [00:01<00:00, 16.33it/s, acc=0.515, loss=1.83] \n",
      "Epoch 4 (Train): 100%|██████████| 171/171 [00:18<00:00,  9.31it/s, acc=0.873, loss=0.343]\n",
      "Epoch 4 (Val): 100%|██████████| 22/22 [00:01<00:00, 17.47it/s, acc=0.515, loss=1.66] \n",
      "Epoch 5 (Train): 100%|██████████| 171/171 [00:17<00:00,  9.73it/s, acc=0.838, loss=0.406]\n",
      "Epoch 5 (Val): 100%|██████████| 22/22 [00:01<00:00, 17.11it/s, acc=0.515, loss=1.55] \n",
      "Epoch 6 (Train): 100%|██████████| 171/171 [00:17<00:00,  9.65it/s, acc=0.82, loss=0.434] \n",
      "Epoch 6 (Val): 100%|██████████| 22/22 [00:01<00:00, 17.01it/s, acc=0.515, loss=1.43] \n",
      "Epoch 7 (Train): 100%|██████████| 171/171 [00:18<00:00,  9.42it/s, acc=0.809, loss=0.449]\n",
      "Epoch 7 (Val): 100%|██████████| 22/22 [00:01<00:00, 16.95it/s, acc=0.515, loss=1.39] \n",
      "Epoch 8 (Train): 100%|██████████| 171/171 [00:17<00:00,  9.73it/s, acc=0.785, loss=0.487]\n",
      "Epoch 8 (Val): 100%|██████████| 22/22 [00:01<00:00, 17.42it/s, acc=0.515, loss=1.3]  \n",
      "Epoch 9 (Train): 100%|██████████| 171/171 [00:08<00:00, 20.20it/s, acc=0.762, loss=0.519]\n",
      "Epoch 9 (Val): 100%|██████████| 22/22 [00:00<00:00, 44.22it/s, acc=0.515, loss=1.22] \n",
      "Epoch 10 (Train): 100%|██████████| 171/171 [00:06<00:00, 25.46it/s, acc=0.744, loss=0.544]\n",
      "Epoch 10 (Val): 100%|██████████| 22/22 [00:00<00:00, 46.65it/s, acc=0.515, loss=1.15]\n",
      "Epoch 11 (Train): 100%|██████████| 171/171 [00:06<00:00, 26.08it/s, acc=0.727, loss=0.563]\n",
      "Epoch 11 (Val): 100%|██████████| 22/22 [00:00<00:00, 45.70it/s, acc=0.515, loss=1.1]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- batch_size_100; lr_0.005; optimizer_Adagrad; hidden_dim_16; num_layers_2 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Train): 100%|██████████| 86/86 [00:04<00:00, 18.30it/s, acc=0.787, loss=0.513]\n",
      "Epoch 1 (Val): 100%|██████████| 11/11 [00:00<00:00, 21.90it/s, acc=0.515, loss=0.936]\n",
      "Epoch 2 (Train): 100%|██████████| 86/86 [00:04<00:00, 19.71it/s, acc=0.612, loss=0.691]\n",
      "Epoch 2 (Val): 100%|██████████| 11/11 [00:00<00:00, 31.49it/s, acc=0.515, loss=0.798]\n",
      "Epoch 3 (Train): 100%|██████████| 86/86 [00:04<00:00, 20.39it/s, acc=0.577, loss=0.695] \n",
      "Epoch 3 (Val): 100%|██████████| 11/11 [00:00<00:00, 29.73it/s, acc=0.515, loss=0.751]\n",
      "Epoch 4 (Train): 100%|██████████| 86/86 [00:04<00:00, 19.01it/s, acc=0.554, loss=0.695] \n",
      "Epoch 4 (Val): 100%|██████████| 11/11 [00:00<00:00, 30.44it/s, acc=0.515, loss=0.73]\n",
      "Epoch 5 (Train): 100%|██████████| 86/86 [00:04<00:00, 18.78it/s, acc=0.554, loss=0.695] \n",
      "Epoch 5 (Val): 100%|██████████| 11/11 [00:00<00:00, 30.23it/s, acc=0.515, loss=0.719]\n",
      "Epoch 6 (Train): 100%|██████████| 86/86 [00:04<00:00, 18.82it/s, acc=0.531, loss=0.695] \n",
      "Epoch 6 (Val): 100%|██████████| 11/11 [00:00<00:00, 26.03it/s, acc=0.515, loss=0.712]\n",
      "Epoch 7 (Train): 100%|██████████| 86/86 [00:04<00:00, 19.37it/s, acc=0.531, loss=0.695] \n",
      "Epoch 7 (Val): 100%|██████████| 11/11 [00:00<00:00, 27.93it/s, acc=0.515, loss=0.708]\n",
      "Epoch 8 (Train): 100%|██████████| 86/86 [00:04<00:00, 19.56it/s, acc=0.531, loss=0.695] \n",
      "Epoch 8 (Val): 100%|██████████| 11/11 [00:00<00:00, 27.88it/s, acc=0.515, loss=0.705]\n",
      "Epoch 9 (Train): 100%|██████████| 86/86 [00:09<00:00,  8.82it/s, acc=0.519, loss=0.695] \n",
      "Epoch 9 (Val): 100%|██████████| 11/11 [00:01<00:00, 10.84it/s, acc=0.515, loss=0.703]\n",
      "Epoch 10 (Train): 100%|██████████| 86/86 [00:10<00:00,  8.42it/s, acc=0.519, loss=0.695] \n",
      "Epoch 10 (Val): 100%|██████████| 11/11 [00:00<00:00, 13.61it/s, acc=0.515, loss=0.701]\n",
      "Epoch 11 (Train): 100%|██████████| 86/86 [00:10<00:00,  8.35it/s, acc=0.519, loss=0.695] \n",
      "Epoch 11 (Val): 100%|██████████| 11/11 [00:01<00:00, 10.78it/s, acc=0.515, loss=0.7]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- batch_size_100; lr_0.01; optimizer_Adagrad; hidden_dim_16; num_layers_2 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Train): 100%|██████████| 86/86 [00:11<00:00,  7.18it/s, acc=0.915, loss=0.294] \n",
      "Epoch 1 (Val): 100%|██████████| 11/11 [00:01<00:00, 10.35it/s, acc=0.515, loss=1.32]\n",
      "Epoch 2 (Train): 100%|██████████| 86/86 [00:11<00:00,  7.51it/s, acc=0.763, loss=0.555]\n",
      "Epoch 2 (Val): 100%|██████████| 11/11 [00:01<00:00, 10.94it/s, acc=0.515, loss=1.08]\n",
      "Epoch 3 (Train): 100%|██████████| 86/86 [00:11<00:00,  7.55it/s, acc=0.694, loss=0.621]\n",
      "Epoch 3 (Val): 100%|██████████| 11/11 [00:00<00:00, 11.14it/s, acc=0.515, loss=0.957]\n",
      "Epoch 4 (Train): 100%|██████████| 86/86 [00:11<00:00,  7.21it/s, acc=0.647, loss=0.654]\n",
      "Epoch 4 (Val): 100%|██████████| 11/11 [00:01<00:00, 10.35it/s, acc=0.515, loss=0.87]\n",
      "Epoch 5 (Train): 100%|██████████| 86/86 [00:11<00:00,  7.34it/s, acc=0.612, loss=0.673]\n",
      "Epoch 5 (Val): 100%|██████████| 11/11 [00:01<00:00, 10.58it/s, acc=0.515, loss=0.817]\n",
      "Epoch 6 (Train): 100%|██████████| 86/86 [00:10<00:00,  8.32it/s, acc=0.601, loss=0.681] \n",
      "Epoch 6 (Val): 100%|██████████| 11/11 [00:00<00:00, 11.13it/s, acc=0.515, loss=0.782]\n",
      "Epoch 7 (Train): 100%|██████████| 86/86 [00:11<00:00,  7.43it/s, acc=0.577, loss=0.686] \n",
      "Epoch 7 (Val): 100%|██████████| 11/11 [00:00<00:00, 12.34it/s, acc=0.515, loss=0.76] \n",
      "Epoch 8 (Train): 100%|██████████| 86/86 [00:11<00:00,  7.38it/s, acc=0.566, loss=0.688] \n",
      "Epoch 8 (Val): 100%|██████████| 11/11 [00:00<00:00, 11.51it/s, acc=0.515, loss=0.744]\n",
      "Epoch 9 (Train): 100%|██████████| 86/86 [00:12<00:00,  7.13it/s, acc=0.554, loss=0.69]  \n",
      "Epoch 9 (Val): 100%|██████████| 11/11 [00:01<00:00, 10.83it/s, acc=0.515, loss=0.733]\n",
      "Epoch 10 (Train): 100%|██████████| 86/86 [00:12<00:00,  6.98it/s, acc=0.554, loss=0.691]\n",
      "Epoch 10 (Val): 100%|██████████| 11/11 [00:00<00:00, 15.44it/s, acc=0.515, loss=0.726]\n",
      "Epoch 11 (Train): 100%|██████████| 86/86 [00:10<00:00,  8.13it/s, acc=0.554, loss=0.692] \n",
      "Epoch 11 (Val): 100%|██████████| 11/11 [00:00<00:00, 11.56it/s, acc=0.515, loss=0.72] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- batch_size_100; lr_0.025; optimizer_Adagrad; hidden_dim_16; num_layers_2 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Train): 100%|██████████| 86/86 [00:11<00:00,  7.26it/s, acc=0.949, loss=0.183] \n",
      "Epoch 1 (Val): 100%|██████████| 11/11 [00:00<00:00, 13.29it/s, acc=0.515, loss=1.81]\n",
      "Epoch 2 (Train): 100%|██████████| 86/86 [00:11<00:00,  7.33it/s, acc=0.856, loss=0.408]\n",
      "Epoch 2 (Val): 100%|██████████| 11/11 [00:00<00:00, 11.01it/s, acc=0.515, loss=1.44]\n",
      "Epoch 3 (Train): 100%|██████████| 86/86 [00:11<00:00,  7.49it/s, acc=0.787, loss=0.519]\n",
      "Epoch 3 (Val): 100%|██████████| 11/11 [00:00<00:00, 11.05it/s, acc=0.515, loss=1.17]\n",
      "Epoch 4 (Train): 100%|██████████| 86/86 [00:11<00:00,  7.33it/s, acc=0.728, loss=0.583]\n",
      "Epoch 4 (Val): 100%|██████████| 11/11 [00:01<00:00, 10.79it/s, acc=0.515, loss=1.03]\n",
      "Epoch 5 (Train): 100%|██████████| 86/86 [00:11<00:00,  7.40it/s, acc=0.682, loss=0.624]\n",
      "Epoch 5 (Val): 100%|██████████| 11/11 [00:01<00:00, 10.98it/s, acc=0.515, loss=0.927]\n",
      "Epoch 6 (Train): 100%|██████████| 86/86 [00:11<00:00,  7.38it/s, acc=0.647, loss=0.65] \n",
      "Epoch 6 (Val): 100%|██████████| 11/11 [00:01<00:00, 10.98it/s, acc=0.515, loss=0.853]\n",
      "Epoch 7 (Train): 100%|██████████| 86/86 [00:11<00:00,  7.54it/s, acc=0.624, loss=0.662]\n",
      "Epoch 7 (Val): 100%|██████████| 11/11 [00:00<00:00, 11.66it/s, acc=0.515, loss=0.813]\n",
      "Epoch 8 (Train): 100%|██████████| 86/86 [00:10<00:00,  7.88it/s, acc=0.612, loss=0.668] \n",
      "Epoch 8 (Val): 100%|██████████| 11/11 [00:00<00:00, 11.70it/s, acc=0.515, loss=0.79] \n",
      "Epoch 9 (Train): 100%|██████████| 86/86 [00:09<00:00,  8.93it/s, acc=0.601, loss=0.673] \n",
      "Epoch 9 (Val): 100%|██████████| 11/11 [00:00<00:00, 11.16it/s, acc=0.515, loss=0.774]\n",
      "Epoch 10 (Train): 100%|██████████| 86/86 [00:11<00:00,  7.37it/s, acc=0.589, loss=0.677]\n",
      "Epoch 10 (Val): 100%|██████████| 11/11 [00:00<00:00, 11.27it/s, acc=0.515, loss=0.762]\n",
      "Epoch 11 (Train): 100%|██████████| 86/86 [00:10<00:00,  8.47it/s, acc=0.589, loss=0.681] \n",
      "Epoch 11 (Val): 100%|██████████| 11/11 [00:00<00:00, 24.51it/s, acc=0.515, loss=0.752]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- batch_size_100; lr_0.05; optimizer_Adagrad; hidden_dim_16; num_layers_2 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Train): 100%|██████████| 86/86 [00:05<00:00, 16.19it/s, acc=0.973, loss=0.127] \n",
      "Epoch 1 (Val): 100%|██████████| 11/11 [00:00<00:00, 24.86it/s, acc=0.515, loss=2.27]\n",
      "Epoch 2 (Train): 100%|██████████| 86/86 [00:04<00:00, 18.97it/s, acc=0.891, loss=0.318]\n",
      "Epoch 2 (Val): 100%|██████████| 11/11 [00:00<00:00, 29.43it/s, acc=0.515, loss=1.73]\n",
      "Epoch 3 (Train): 100%|██████████| 86/86 [00:04<00:00, 19.50it/s, acc=0.856, loss=0.389]\n",
      "Epoch 3 (Val): 100%|██████████| 11/11 [00:00<00:00, 30.16it/s, acc=0.515, loss=1.54]\n",
      "Epoch 4 (Train): 100%|██████████| 86/86 [00:04<00:00, 17.94it/s, acc=0.81, loss=0.46]  \n",
      "Epoch 4 (Val): 100%|██████████| 11/11 [00:00<00:00, 27.55it/s, acc=0.515, loss=1.37]\n",
      "Epoch 5 (Train): 100%|██████████| 86/86 [00:04<00:00, 18.81it/s, acc=0.775, loss=0.503]\n",
      "Epoch 5 (Val): 100%|██████████| 11/11 [00:00<00:00, 29.77it/s, acc=0.515, loss=1.32]\n",
      "Epoch 6 (Train): 100%|██████████| 86/86 [00:04<00:00, 19.79it/s, acc=0.752, loss=0.532]\n",
      "Epoch 6 (Val): 100%|██████████| 11/11 [00:00<00:00, 31.61it/s, acc=0.515, loss=1.26]\n",
      "Epoch 7 (Train): 100%|██████████| 86/86 [00:03<00:00, 21.59it/s, acc=0.728, loss=0.564]\n",
      "Epoch 7 (Val): 100%|██████████| 11/11 [00:00<00:00, 32.42it/s, acc=0.515, loss=1.13]\n",
      "Epoch 8 (Train): 100%|██████████| 86/86 [00:04<00:00, 19.70it/s, acc=0.705, loss=0.591]\n",
      "Epoch 8 (Val): 100%|██████████| 11/11 [00:00<00:00, 31.18it/s, acc=0.515, loss=1.08]\n",
      "Epoch 9 (Train): 100%|██████████| 86/86 [00:04<00:00, 20.37it/s, acc=0.694, loss=0.603]\n",
      "Epoch 9 (Val): 100%|██████████| 11/11 [00:00<00:00, 32.10it/s, acc=0.515, loss=1.05]\n",
      "Epoch 10 (Train): 100%|██████████| 86/86 [00:04<00:00, 20.34it/s, acc=0.682, loss=0.624]\n",
      "Epoch 10 (Val): 100%|██████████| 11/11 [00:00<00:00, 33.34it/s, acc=0.515, loss=0.984]\n",
      "Epoch 11 (Train): 100%|██████████| 86/86 [00:03<00:00, 21.69it/s, acc=0.659, loss=0.637]\n",
      "Epoch 11 (Val): 100%|██████████| 11/11 [00:00<00:00, 32.93it/s, acc=0.515, loss=0.932]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- batch_size_200; lr_0.005; optimizer_Adagrad; hidden_dim_16; num_layers_2 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Train): 100%|██████████| 43/43 [00:03<00:00, 12.73it/s, acc=0.682, loss=0.647]\n",
      "Epoch 1 (Val): 100%|██████████| 6/6 [00:00<00:00, 17.75it/s, acc=0.556, loss=0.733]\n",
      "Epoch 2 (Train): 100%|██████████| 43/43 [00:03<00:00, 12.02it/s, acc=0.566, loss=0.707]\n",
      "Epoch 2 (Val): 100%|██████████| 6/6 [00:00<00:00, 18.38it/s, acc=0.556, loss=0.702]\n",
      "Epoch 3 (Train): 100%|██████████| 43/43 [00:03<00:00, 12.46it/s, acc=0.519, loss=0.703] \n",
      "Epoch 3 (Val): 100%|██████████| 6/6 [00:00<00:00, 19.42it/s, acc=0.556, loss=0.694]\n",
      "Epoch 4 (Train): 100%|██████████| 43/43 [00:03<00:00, 12.95it/s, acc=0.519, loss=0.701]\n",
      "Epoch 4 (Val): 100%|██████████| 6/6 [00:00<00:00, 18.07it/s, acc=0.556, loss=0.69] \n",
      "Epoch 5 (Train): 100%|██████████| 43/43 [00:03<00:00, 11.52it/s, acc=0.519, loss=0.7]   \n",
      "Epoch 5 (Val): 100%|██████████| 6/6 [00:00<00:00, 15.82it/s, acc=0.556, loss=0.689]\n",
      "Epoch 6 (Train): 100%|██████████| 43/43 [00:04<00:00,  9.84it/s, acc=0.519, loss=0.699] \n",
      "Epoch 6 (Val): 100%|██████████| 6/6 [00:00<00:00, 12.09it/s, acc=0.556, loss=0.688]\n",
      "Epoch 7 (Train): 100%|██████████| 43/43 [00:08<00:00,  4.82it/s, acc=0.519, loss=0.698] \n",
      "Epoch 7 (Val): 100%|██████████| 6/6 [00:01<00:00,  5.98it/s, acc=0.556, loss=0.687]\n",
      "Epoch 8 (Train): 100%|██████████| 43/43 [00:09<00:00,  4.32it/s, acc=0.519, loss=0.698] \n",
      "Epoch 8 (Val): 100%|██████████| 6/6 [00:00<00:00,  7.23it/s, acc=0.556, loss=0.687]\n",
      "Epoch 9 (Train): 100%|██████████| 43/43 [00:08<00:00,  5.05it/s, acc=0.519, loss=0.698] \n",
      "Epoch 9 (Val): 100%|██████████| 6/6 [00:00<00:00,  6.87it/s, acc=0.556, loss=0.687]\n",
      "Epoch 10 (Train): 100%|██████████| 43/43 [00:09<00:00,  4.64it/s, acc=0.519, loss=0.697] \n",
      "Epoch 10 (Val): 100%|██████████| 6/6 [00:00<00:00, 17.44it/s, acc=0.556, loss=0.687]\n",
      "Epoch 11 (Train): 100%|██████████| 43/43 [00:09<00:00,  4.70it/s, acc=0.496, loss=0.697] \n",
      "Epoch 11 (Val): 100%|██████████| 6/6 [00:00<00:00,  6.46it/s, acc=0.556, loss=0.687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- batch_size_200; lr_0.01; optimizer_Adagrad; hidden_dim_16; num_layers_2 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Train): 100%|██████████| 43/43 [00:09<00:00,  4.64it/s, acc=0.868, loss=0.413]\n",
      "Epoch 1 (Val): 100%|██████████| 6/6 [00:01<00:00,  5.76it/s, acc=0.556, loss=1.1] \n",
      "Epoch 2 (Train): 100%|██████████| 43/43 [00:09<00:00,  4.41it/s, acc=0.659, loss=0.691]\n",
      "Epoch 2 (Val): 100%|██████████| 6/6 [00:00<00:00,  6.36it/s, acc=0.556, loss=0.835]\n",
      "Epoch 3 (Train): 100%|██████████| 43/43 [00:09<00:00,  4.36it/s, acc=0.589, loss=0.701]\n",
      "Epoch 3 (Val): 100%|██████████| 6/6 [00:01<00:00,  5.98it/s, acc=0.556, loss=0.756]\n",
      "Epoch 4 (Train): 100%|██████████| 43/43 [00:08<00:00,  4.93it/s, acc=0.566, loss=0.701]\n",
      "Epoch 4 (Val): 100%|██████████| 6/6 [00:00<00:00,  6.07it/s, acc=0.556, loss=0.724]\n",
      "Epoch 5 (Train): 100%|██████████| 43/43 [00:09<00:00,  4.45it/s, acc=0.542, loss=0.7]  \n",
      "Epoch 5 (Val): 100%|██████████| 6/6 [00:01<00:00,  5.95it/s, acc=0.556, loss=0.709]\n",
      "Epoch 6 (Train): 100%|██████████| 43/43 [00:09<00:00,  4.33it/s, acc=0.542, loss=0.7]   \n",
      "Epoch 6 (Val): 100%|██████████| 6/6 [00:00<00:00,  6.52it/s, acc=0.556, loss=0.701]\n",
      "Epoch 7 (Train): 100%|██████████| 43/43 [00:09<00:00,  4.68it/s, acc=0.542, loss=0.699] \n",
      "Epoch 7 (Val): 100%|██████████| 6/6 [00:00<00:00,  7.78it/s, acc=0.556, loss=0.696]\n",
      "Epoch 8 (Train): 100%|██████████| 43/43 [00:09<00:00,  4.31it/s, acc=0.542, loss=0.699] \n",
      "Epoch 8 (Val): 100%|██████████| 6/6 [00:00<00:00,  6.58it/s, acc=0.556, loss=0.693]\n",
      "Epoch 9 (Train): 100%|██████████| 43/43 [00:09<00:00,  4.42it/s, acc=0.542, loss=0.699] \n",
      "Epoch 9 (Val): 100%|██████████| 6/6 [00:00<00:00,  6.09it/s, acc=0.556, loss=0.691]\n",
      "Epoch 10 (Train): 100%|██████████| 43/43 [00:08<00:00,  5.20it/s, acc=0.519, loss=0.698] \n",
      "Epoch 10 (Val): 100%|██████████| 6/6 [00:00<00:00,  6.28it/s, acc=0.556, loss=0.69] \n",
      "Epoch 11 (Train): 100%|██████████| 43/43 [00:09<00:00,  4.44it/s, acc=0.519, loss=0.698]\n",
      "Epoch 11 (Val): 100%|██████████| 6/6 [00:00<00:00,  6.45it/s, acc=0.556, loss=0.689]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- batch_size_200; lr_0.025; optimizer_Adagrad; hidden_dim_16; num_layers_2 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Train): 100%|██████████| 43/43 [00:06<00:00,  6.98it/s, acc=0.891, loss=0.312] \n",
      "Epoch 1 (Val): 100%|██████████| 6/6 [00:00<00:00, 14.63it/s, acc=0.556, loss=1.48]\n",
      "Epoch 2 (Train): 100%|██████████| 43/43 [00:07<00:00,  6.09it/s, acc=0.728, loss=0.637]\n",
      "Epoch 2 (Val): 100%|██████████| 6/6 [00:01<00:00,  5.95it/s, acc=0.556, loss=1.03]\n",
      "Epoch 3 (Train): 100%|██████████| 43/43 [00:10<00:00,  4.29it/s, acc=0.635, loss=0.697]\n",
      "Epoch 3 (Val): 100%|██████████| 6/6 [00:01<00:00,  5.90it/s, acc=0.556, loss=0.847]\n",
      "Epoch 4 (Train): 100%|██████████| 43/43 [00:09<00:00,  4.34it/s, acc=0.589, loss=0.702]\n",
      "Epoch 4 (Val): 100%|██████████| 6/6 [00:01<00:00,  5.88it/s, acc=0.556, loss=0.768]\n",
      "Epoch 5 (Train): 100%|██████████| 43/43 [00:05<00:00,  7.37it/s, acc=0.542, loss=0.703]\n",
      "Epoch 5 (Val): 100%|██████████| 6/6 [00:00<00:00, 15.34it/s, acc=0.556, loss=0.724]\n",
      "Epoch 6 (Train): 100%|██████████| 43/43 [00:04<00:00, 10.58it/s, acc=0.542, loss=0.702]\n",
      "Epoch 6 (Val): 100%|██████████| 6/6 [00:00<00:00, 14.39it/s, acc=0.556, loss=0.705]\n",
      "Epoch 7 (Train): 100%|██████████| 43/43 [00:03<00:00, 11.03it/s, acc=0.519, loss=0.701] \n",
      "Epoch 7 (Val): 100%|██████████| 6/6 [00:00<00:00, 16.77it/s, acc=0.556, loss=0.696]\n",
      "Epoch 8 (Train): 100%|██████████| 43/43 [00:04<00:00, 10.34it/s, acc=0.519, loss=0.7]   \n",
      "Epoch 8 (Val): 100%|██████████| 6/6 [00:00<00:00, 16.23it/s, acc=0.556, loss=0.692]\n",
      "Epoch 9 (Train): 100%|██████████| 43/43 [00:03<00:00, 11.21it/s, acc=0.519, loss=0.699] \n",
      "Epoch 9 (Val): 100%|██████████| 6/6 [00:00<00:00,  6.05it/s, acc=0.556, loss=0.689]\n",
      "Epoch 10 (Train): 100%|██████████| 43/43 [00:04<00:00,  8.94it/s, acc=0.519, loss=0.699] \n",
      "Epoch 10 (Val): 100%|██████████| 6/6 [00:00<00:00, 14.07it/s, acc=0.556, loss=0.688]\n",
      "Epoch 11 (Train): 100%|██████████| 43/43 [00:03<00:00, 10.77it/s, acc=0.496, loss=0.698] \n",
      "Epoch 11 (Val): 100%|██████████| 6/6 [00:00<00:00, 16.83it/s, acc=0.556, loss=0.687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- batch_size_200; lr_0.05; optimizer_Adagrad; hidden_dim_16; num_layers_2 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Train): 100%|██████████| 43/43 [00:04<00:00,  9.37it/s, acc=0.915, loss=0.239] \n",
      "Epoch 1 (Val): 100%|██████████| 6/6 [00:00<00:00, 13.36it/s, acc=0.556, loss=1.81]\n",
      "Epoch 2 (Train):  19%|█▊        | 8/43 [00:00<00:03, 10.38it/s, acc=0.625, loss=1.17]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m learning_rate \u001b[38;5;129;01min\u001b[39;00m SEARCH_SPACE[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m      8\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---------- batch_size_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; lr_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; optimizer_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimizer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; hidden_dim_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; num_layers_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ----------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m   configuration_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_rnn_model_with_parameters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_layers\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m   all_configurations_results\u001b[38;5;241m.\u001b[39mappend(configuration_results)\n",
      "Cell \u001b[1;32mIn[4], line 66\u001b[0m, in \u001b[0;36mtrain_rnn_model_with_parameters\u001b[1;34m(batch_size, learning_rate, optimizer_name, hidden_dim, num_layers)\u001b[0m\n\u001b[0;32m     61\u001b[0m val_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m########################\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m######### Train ########\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m########################\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m model, avg_train_loss, avg_train_acc, avg_val_loss, avg_val_acc, num_of_epochs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_rnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m  \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m  \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m  \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmax_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_epochs\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m########################\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m######### Plot #########\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m########################\u001b[39;00m\n\u001b[0;32m     78\u001b[0m subtitle \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; lr_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; optimizer_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimizer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; hidden_dim_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; num_layers_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Toh Jing Qiang\\Desktop\\sc4002-nlp-sentiment-classification\\solver.py:41\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, criterion, optimizer, train_dataloader, val_dataloader, max_epoch, max_non_increasing_epoch_count)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     39\u001b[0m     num_of_epochs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 41\u001b[0m     epoch_train_loss, epoch_train_acc \u001b[38;5;241m=\u001b[39m \u001b[43m__train_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     epoch_val_loss, epoch_val_acc \u001b[38;5;241m=\u001b[39m __validate(model, criterion, val_dataloader, epoch)\n\u001b[0;32m     44\u001b[0m     avg_train_loss\u001b[38;5;241m.\u001b[39mappend(epoch_train_loss)\n",
      "File \u001b[1;32mc:\\Users\\Toh Jing Qiang\\Desktop\\sc4002-nlp-sentiment-classification\\solver.py:77\u001b[0m, in \u001b[0;36m__train_one_epoch\u001b[1;34m(model, train_dataloader, optimizer, criterion, epoch)\u001b[0m\n\u001b[0;32m     74\u001b[0m t\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m (Train)\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m epoch)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Mini-Batch\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Toh Jing Qiang\\Desktop\\sc4002-nlp-sentiment-classification\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Toh Jing Qiang\\Desktop\\sc4002-nlp-sentiment-classification\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Toh Jing Qiang\\Desktop\\sc4002-nlp-sentiment-classification\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Toh Jing Qiang\\Desktop\\sc4002-nlp-sentiment-classification\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Toh Jing Qiang\\Desktop\\sc4002-nlp-sentiment-classification\\utils\\custom_dataset.py:42\u001b[0m, in \u001b[0;36mTextDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Convert to PyTorch tensors\u001b[39;00m\n\u001b[0;32m     41\u001b[0m sentence \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(sentence_tokens_indexes, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m---> 42\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sentence, label\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_configurations_results = []\n",
    "\n",
    "for hidden_dim in SEARCH_SPACE[\"hidden_dim\"]:\n",
    "  for num_layers in SEARCH_SPACE[\"num_layers\"]:\n",
    "    for optimizer_name in SEARCH_SPACE[\"optimizer_name\"]:\n",
    "      for batch_size in SEARCH_SPACE[\"batch_size\"]:\n",
    "        for learning_rate in SEARCH_SPACE[\"learning_rate\"]:\n",
    "          print(f\"---------- batch_size_{batch_size}; lr_{learning_rate}; optimizer_{optimizer_name}; hidden_dim_{hidden_dim}; num_layers_{num_layers} ----------\")\n",
    "          configuration_results = train_rnn_model_with_parameters(\n",
    "            batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            optimizer_name=optimizer_name,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=num_layers\n",
    "          )\n",
    "          all_configurations_results.append(configuration_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Configurations Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>optimizer_name</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>num_of_epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>0.050</td>\n",
       "      <td>SGD</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.692284</td>\n",
       "      <td>0.565698</td>\n",
       "      <td>0.716110</td>\n",
       "      <td>0.555833</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200</td>\n",
       "      <td>0.005</td>\n",
       "      <td>SGD</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.696331</td>\n",
       "      <td>0.519186</td>\n",
       "      <td>0.687343</td>\n",
       "      <td>0.555833</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.698037</td>\n",
       "      <td>0.519186</td>\n",
       "      <td>0.688852</td>\n",
       "      <td>0.555833</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200</td>\n",
       "      <td>0.005</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.697021</td>\n",
       "      <td>0.495930</td>\n",
       "      <td>0.686907</td>\n",
       "      <td>0.555833</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>0.025</td>\n",
       "      <td>SGD</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.695644</td>\n",
       "      <td>0.542442</td>\n",
       "      <td>0.696105</td>\n",
       "      <td>0.555833</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200</td>\n",
       "      <td>0.010</td>\n",
       "      <td>SGD</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.696223</td>\n",
       "      <td>0.519186</td>\n",
       "      <td>0.690097</td>\n",
       "      <td>0.555833</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>200</td>\n",
       "      <td>0.025</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.698202</td>\n",
       "      <td>0.495930</td>\n",
       "      <td>0.687463</td>\n",
       "      <td>0.555833</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100</td>\n",
       "      <td>0.050</td>\n",
       "      <td>SGD</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.455782</td>\n",
       "      <td>0.798256</td>\n",
       "      <td>1.509111</td>\n",
       "      <td>0.515455</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100</td>\n",
       "      <td>0.025</td>\n",
       "      <td>SGD</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.672918</td>\n",
       "      <td>0.588953</td>\n",
       "      <td>0.794848</td>\n",
       "      <td>0.515455</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100</td>\n",
       "      <td>0.010</td>\n",
       "      <td>SGD</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.689455</td>\n",
       "      <td>0.554070</td>\n",
       "      <td>0.719162</td>\n",
       "      <td>0.515455</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100</td>\n",
       "      <td>0.005</td>\n",
       "      <td>SGD</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.689774</td>\n",
       "      <td>0.542442</td>\n",
       "      <td>0.713568</td>\n",
       "      <td>0.515455</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>50</td>\n",
       "      <td>0.010</td>\n",
       "      <td>SGD</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.654517</td>\n",
       "      <td>0.621637</td>\n",
       "      <td>0.863456</td>\n",
       "      <td>0.515455</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>50</td>\n",
       "      <td>0.005</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.685741</td>\n",
       "      <td>0.563158</td>\n",
       "      <td>0.724301</td>\n",
       "      <td>0.515455</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>50</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.638161</td>\n",
       "      <td>0.650877</td>\n",
       "      <td>0.876675</td>\n",
       "      <td>0.515455</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>50</td>\n",
       "      <td>0.025</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.648008</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.859363</td>\n",
       "      <td>0.515455</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>50</td>\n",
       "      <td>0.050</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.563097</td>\n",
       "      <td>0.726901</td>\n",
       "      <td>1.095382</td>\n",
       "      <td>0.515455</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>100</td>\n",
       "      <td>0.005</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.694520</td>\n",
       "      <td>0.519186</td>\n",
       "      <td>0.699776</td>\n",
       "      <td>0.515455</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>100</td>\n",
       "      <td>0.010</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.691756</td>\n",
       "      <td>0.554070</td>\n",
       "      <td>0.719962</td>\n",
       "      <td>0.515455</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>100</td>\n",
       "      <td>0.025</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.680736</td>\n",
       "      <td>0.588953</td>\n",
       "      <td>0.751876</td>\n",
       "      <td>0.515455</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>100</td>\n",
       "      <td>0.050</td>\n",
       "      <td>Adagrad</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.637488</td>\n",
       "      <td>0.658721</td>\n",
       "      <td>0.932225</td>\n",
       "      <td>0.515455</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>50</td>\n",
       "      <td>0.050</td>\n",
       "      <td>SGD</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.299609</td>\n",
       "      <td>0.884795</td>\n",
       "      <td>2.004038</td>\n",
       "      <td>0.515455</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>50</td>\n",
       "      <td>0.025</td>\n",
       "      <td>SGD</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.447501</td>\n",
       "      <td>0.797076</td>\n",
       "      <td>1.452172</td>\n",
       "      <td>0.515455</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>50</td>\n",
       "      <td>0.005</td>\n",
       "      <td>SGD</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.671400</td>\n",
       "      <td>0.592398</td>\n",
       "      <td>0.764233</td>\n",
       "      <td>0.515455</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    batch_size  learning_rate optimizer_name  hidden_dim  num_layers  \\\n",
       "0          200          0.050            SGD          16           2   \n",
       "1          200          0.005            SGD          16           2   \n",
       "2          200          0.010        Adagrad          16           2   \n",
       "3          200          0.005        Adagrad          16           2   \n",
       "4          200          0.025            SGD          16           2   \n",
       "5          200          0.010            SGD          16           2   \n",
       "6          200          0.025        Adagrad          16           2   \n",
       "7          100          0.050            SGD          16           2   \n",
       "8          100          0.025            SGD          16           2   \n",
       "9          100          0.010            SGD          16           2   \n",
       "10         100          0.005            SGD          16           2   \n",
       "11          50          0.010            SGD          16           2   \n",
       "12          50          0.005        Adagrad          16           2   \n",
       "13          50          0.010        Adagrad          16           2   \n",
       "14          50          0.025        Adagrad          16           2   \n",
       "15          50          0.050        Adagrad          16           2   \n",
       "16         100          0.005        Adagrad          16           2   \n",
       "17         100          0.010        Adagrad          16           2   \n",
       "18         100          0.025        Adagrad          16           2   \n",
       "19         100          0.050        Adagrad          16           2   \n",
       "20          50          0.050            SGD          16           2   \n",
       "21          50          0.025            SGD          16           2   \n",
       "22          50          0.005            SGD          16           2   \n",
       "\n",
       "    train_loss  train_accuracy  val_loss  val_accuracy  num_of_epochs  \n",
       "0     0.692284        0.565698  0.716110      0.555833             11  \n",
       "1     0.696331        0.519186  0.687343      0.555833             11  \n",
       "2     0.698037        0.519186  0.688852      0.555833             11  \n",
       "3     0.697021        0.495930  0.686907      0.555833             11  \n",
       "4     0.695644        0.542442  0.696105      0.555833             11  \n",
       "5     0.696223        0.519186  0.690097      0.555833             11  \n",
       "6     0.698202        0.495930  0.687463      0.555833             11  \n",
       "7     0.455782        0.798256  1.509111      0.515455             11  \n",
       "8     0.672918        0.588953  0.794848      0.515455             11  \n",
       "9     0.689455        0.554070  0.719162      0.515455             11  \n",
       "10    0.689774        0.542442  0.713568      0.515455             11  \n",
       "11    0.654517        0.621637  0.863456      0.515455             11  \n",
       "12    0.685741        0.563158  0.724301      0.515455             11  \n",
       "13    0.638161        0.650877  0.876675      0.515455             11  \n",
       "14    0.648008        0.633333  0.859363      0.515455             11  \n",
       "15    0.563097        0.726901  1.095382      0.515455             11  \n",
       "16    0.694520        0.519186  0.699776      0.515455             11  \n",
       "17    0.691756        0.554070  0.719962      0.515455             11  \n",
       "18    0.680736        0.588953  0.751876      0.515455             11  \n",
       "19    0.637488        0.658721  0.932225      0.515455             11  \n",
       "20    0.299609        0.884795  2.004038      0.515455             11  \n",
       "21    0.447501        0.797076  1.452172      0.515455             11  \n",
       "22    0.671400        0.592398  0.764233      0.515455             11  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_configurations_results_df = pd.DataFrame.from_dict(all_configurations_results)\n",
    "model_configurations_results_df.sort_values(by=[\"val_accuracy\"], ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a. Final Configuration of best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b. Accuracy on Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c. Strategies to derive final sentence representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
