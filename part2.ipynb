{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading object from local...\n",
      "Object loaded from local!\n",
      "Loading object from local...\n",
      "Object loaded from local!\n"
     ]
    }
   ],
   "source": [
    "# Import Embedding Matrix and Embedding Matrix's Train dataset vocab to index dictionary\n",
    "\n",
    "from utils.file import load_from_local_file\n",
    "\n",
    "embedding_matrix = load_from_local_file(\"models/embedding_matrix.pckl\")\n",
    "embedding_matrix_train_dataset_vocab_to_index: dict = load_from_local_file(\"models/embedding_matrix_train_dataset_vocab_to_index.pckl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the rock is destined to be the 21st century's ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the gorgeously elaborate continuation of \" the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>effective but too-tepid biopic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if you sometimes like to go to the movies to h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>emerges as something rare , an issue movie tha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  the rock is destined to be the 21st century's ...      1\n",
       "1  the gorgeously elaborate continuation of \" the...      1\n",
       "2                     effective but too-tepid biopic      1\n",
       "3  if you sometimes like to go to the movies to h...      1\n",
       "4  emerges as something rare , an issue movie tha...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"datasets/train.csv\")\n",
    "val_df = pd.read_csv(\"datasets/val.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Toh Jing\n",
      "[nltk_data]     Qiang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to C:\\Users\\Toh Jing\n",
      "[nltk_data]     Qiang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Toh Jing\n",
      "[nltk_data]     Qiang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from utils.text import tokenize_sentence\n",
    "\n",
    "# Custom Text Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, max_len):\n",
    "        self.data = dataframe\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the sentence and label\n",
    "        sentence = self.data.iloc[idx, 0]\n",
    "        label = self.data.iloc[idx, 1]\n",
    "\n",
    "        # Tokenize sentence\n",
    "        sentence_tokens: list[str] = tokenize_sentence(sentence)\n",
    "\n",
    "        # Convert Tokens into indexes used in embeddings layer\n",
    "        sentence_tokens_indexes = []\n",
    "        for token in sentence_tokens:\n",
    "            if token in embedding_matrix_train_dataset_vocab_to_index.keys():\n",
    "                sentence_tokens_indexes.append(embedding_matrix_train_dataset_vocab_to_index[token])\n",
    "            else:\n",
    "                # For OOV words in val and test set\n",
    "                sentence_tokens_indexes.append(embedding_matrix_train_dataset_vocab_to_index[\"\"])\n",
    "\n",
    "        # Pad the sentence if it's shorter than max_len, or truncate if it's longer\n",
    "        if len(sentence_tokens_indexes) < self.max_len:\n",
    "            sentence_tokens_indexes = sentence_tokens_indexes + [0] * (self.max_len - len(sentence_tokens_indexes)) # Padding with 0\n",
    "        elif len(sentence_tokens_indexes) > self.max_len:\n",
    "            sentence_tokens_indexes = sentence_tokens_indexes[:self.max_len] # Truncate to max_len\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        sentence = torch.tensor(sentence_tokens_indexes, dtype=torch.long)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return sentence, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 (Train):   0%|          | 0/171 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 (Train): 100%|██████████| 171/171 [00:10<00:00, 16.86it/s, acc=0.955, loss=0.141]\n",
      "Epoch 0 (Val): 100%|██████████| 22/22 [00:00<00:00, 48.96it/s, acc=0.515, loss=2.31] \n",
      "Epoch 1 (Train): 100%|██████████| 171/171 [00:10<00:00, 17.01it/s, acc=0.914, loss=0.23] \n",
      "Epoch 1 (Val): 100%|██████████| 22/22 [00:00<00:00, 49.89it/s, acc=0.515, loss=2.3]  \n",
      "Epoch 2 (Train): 100%|██████████| 171/171 [00:10<00:00, 16.68it/s, acc=0.926, loss=0.213]\n",
      "Epoch 2 (Val): 100%|██████████| 22/22 [00:00<00:00, 49.35it/s, acc=0.515, loss=2.33] \n",
      "Epoch 3 (Train): 100%|██████████| 171/171 [00:10<00:00, 16.70it/s, acc=0.926, loss=0.208]\n",
      "Epoch 3 (Val): 100%|██████████| 22/22 [00:00<00:00, 50.78it/s, acc=0.515, loss=2.34] \n",
      "Epoch 4 (Train): 100%|██████████| 171/171 [00:10<00:00, 16.89it/s, acc=0.926, loss=0.209]\n",
      "Epoch 4 (Val): 100%|██████████| 22/22 [00:00<00:00, 38.98it/s, acc=0.515, loss=2.33] \n",
      "Epoch 5 (Train): 100%|██████████| 171/171 [00:10<00:00, 16.06it/s, acc=0.926, loss=0.213]\n",
      "Epoch 5 (Val): 100%|██████████| 22/22 [00:00<00:00, 49.48it/s, acc=0.515, loss=2.32] \n",
      "Epoch 6 (Train): 100%|██████████| 171/171 [00:10<00:00, 16.67it/s, acc=0.926, loss=0.22] \n",
      "Epoch 6 (Val): 100%|██████████| 22/22 [00:00<00:00, 49.92it/s, acc=0.515, loss=2.3]  \n",
      "Epoch 7 (Train): 100%|██████████| 171/171 [00:10<00:00, 16.67it/s, acc=0.92, loss=0.227] \n",
      "Epoch 7 (Val): 100%|██████████| 22/22 [00:00<00:00, 38.52it/s, acc=0.515, loss=2.28] \n",
      "Epoch 8 (Train): 100%|██████████| 171/171 [00:10<00:00, 16.70it/s, acc=0.914, loss=0.234]\n",
      "Epoch 8 (Val): 100%|██████████| 22/22 [00:00<00:00, 49.08it/s, acc=0.515, loss=2.28] \n",
      "Epoch 9 (Train): 100%|██████████| 171/171 [00:10<00:00, 16.36it/s, acc=0.914, loss=0.234]\n",
      "Epoch 9 (Val): 100%|██████████| 22/22 [00:00<00:00, 50.76it/s, acc=0.515, loss=2.26] \n",
      "Epoch 10 (Train): 100%|██████████| 171/171 [00:10<00:00, 16.49it/s, acc=0.914, loss=0.235]\n",
      "Epoch 10 (Val): 100%|██████████| 22/22 [00:00<00:00, 43.00it/s, acc=0.515, loss=2.27] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(RNN(\n",
       "   (embedding): Embedding(16332, 300)\n",
       "   (rnn): RNN(300, 32, num_layers=8, batch_first=True)\n",
       "   (fc): Linear(in_features=32, out_features=2, bias=True)\n",
       " ),\n",
       " [0.1410138,\n",
       "  0.22987168,\n",
       "  0.21258558,\n",
       "  0.20753047,\n",
       "  0.208947,\n",
       "  0.21326254,\n",
       "  0.21962442,\n",
       "  0.22728582,\n",
       "  0.23414059,\n",
       "  0.23445147,\n",
       "  0.2353951],\n",
       " [0.9549708,\n",
       "  0.9140351,\n",
       "  0.925731,\n",
       "  0.925731,\n",
       "  0.925731,\n",
       "  0.925731,\n",
       "  0.925731,\n",
       "  0.9198831,\n",
       "  0.9140351,\n",
       "  0.9140351,\n",
       "  0.9140351],\n",
       " [2.3114026,\n",
       "  2.3002942,\n",
       "  2.32814,\n",
       "  2.3373911,\n",
       "  2.33325,\n",
       "  2.3207536,\n",
       "  2.3018148,\n",
       "  2.2754319,\n",
       "  2.2796082,\n",
       "  2.2648547,\n",
       "  2.2679617],\n",
       " [0.51545453,\n",
       "  0.51545453,\n",
       "  0.51545453,\n",
       "  0.51545453,\n",
       "  0.51545453,\n",
       "  0.51545453,\n",
       "  0.51545453,\n",
       "  0.51545453,\n",
       "  0.51545453,\n",
       "  0.51545453,\n",
       "  0.51545453])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.RNN import RNN\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from solver import train\n",
    "\n",
    "# Model\n",
    "model_rnn = RNN(\n",
    "  embedding_matrix=embedding_matrix,\n",
    "  hidden_dim=32,\n",
    "  num_layers=8,\n",
    "  output_dim=2\n",
    ")\n",
    "\n",
    "########################\n",
    "###### Parameters ######\n",
    "########################\n",
    "batch_size = 50\n",
    "max_epochs = 10_000\n",
    "\n",
    "# SGD Optimizer\n",
    "learning_rate = 0.05\n",
    "optimizer = torch.optim.SGD(model_rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "# Cross Entropy Loss \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "########################\n",
    "######## Dataset #######\n",
    "########################\n",
    "train_dataset = TextDataset(\n",
    "  dataframe=train_df,\n",
    "  max_len=train_df[\"text\"].str.split().apply(len).max()\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "val_dataset = TextDataset(\n",
    "  dataframe=val_df,\n",
    "  max_len=val_df[\"text\"].str.split().apply(len).max()\n",
    ")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train(\n",
    "  model=model_rnn,\n",
    "  criterion=criterion,\n",
    "  optimizer=optimizer,\n",
    "  train_dataloader=train_dataloader,\n",
    "  val_dataloader=val_dataloader,\n",
    "  max_epoch=max_epochs\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
