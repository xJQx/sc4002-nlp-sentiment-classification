{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Embeddings and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading object from local...\n",
      "Object loaded from local!\n",
      "Loading object from local...\n",
      "Object loaded from local!\n"
     ]
    }
   ],
   "source": [
    "# Import Embedding Matrix and Embedding Matrix's Train dataset vocab to index dictionary\n",
    "from pathlib import Path\n",
    "from utils.file import load_from_local_file\n",
    "\n",
    "embedding_path = Path(\"models/embedding_matrix.pckl\")\n",
    "vocab_to_index_path = Path(\"models/embedding_matrix_train_dataset_vocab_to_index.pckl\")\n",
    "\n",
    "embedding_matrix = load_from_local_file(embedding_path)\n",
    "embedding_matrix_vocab_to_index_train = load_from_local_file(vocab_to_index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the rock is destined to be the 21st century's ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the gorgeously elaborate continuation of \" the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>effective but too-tepid biopic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if you sometimes like to go to the movies to h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>emerges as something rare , an issue movie tha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  the rock is destined to be the 21st century's ...      1\n",
       "1  the gorgeously elaborate continuation of \" the...      1\n",
       "2                     effective but too-tepid biopic      1\n",
       "3  if you sometimes like to go to the movies to h...      1\n",
       "4  emerges as something rare , an issue movie tha...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"datasets/train.csv\")\n",
    "val_df = pd.read_csv(\"datasets/val.csv\")\n",
    "test_df = pd.read_csv(\"datasets/test.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_SPACE = {\n",
    "  \"batch_size\": [32, 64, 128, 256],\n",
    "  \"learning_rate\": [0.001, 0.01, 0.05, 0.1],\n",
    "  \"optimizer_name\": [\"SGD\", \"Adagrad\", \"Adam\", \"RMSprop\"],\n",
    "\n",
    "  # RNN Model Parameters\n",
    "  \"hidden_dim\": [16],\n",
    "  \"num_layers\": [2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/yuri/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to /home/yuri/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/yuri/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from models.RNN import RNN\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from solver import train, plot_loss_acc_graph\n",
    "from utils.custom_dataset import TextDataset\n",
    "\n",
    "def train_rnn_model_with_parameters(\n",
    "    batch_size: int,\n",
    "    learning_rate: float,\n",
    "    optimizer_name: str,\n",
    "    hidden_dim: int,\n",
    "    num_layers: int,\n",
    "    show_progress: bool = True,\n",
    "):\n",
    "  # Model\n",
    "  model_rnn = RNN(\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    output_dim=2,\n",
    "    sentence_representation_type=\"last\"\n",
    "  )\n",
    "\n",
    "  ########################\n",
    "  ###### Parameters ######\n",
    "  ########################\n",
    "  batch_size = batch_size\n",
    "  min_epoch = 10\n",
    "  max_epochs = 10_000\n",
    "\n",
    "  if optimizer_name == \"SGD\":\n",
    "    optimizer = torch.optim.SGD(model_rnn.parameters(), lr=learning_rate)\n",
    "  elif optimizer_name == \"Adagrad\":\n",
    "      optimizer = torch.optim.Adagrad(model_rnn.parameters(), lr=learning_rate)\n",
    "  elif optimizer_name == \"Adam\":\n",
    "      optimizer = torch.optim.Adam(model_rnn.parameters(), lr=learning_rate)\n",
    "  elif optimizer_name == \"RMSprop\":\n",
    "      optimizer = torch.optim.RMSprop(model_rnn.parameters(), lr=learning_rate)\n",
    "  else:\n",
    "      raise Exception(\"Invalid optimizer name!\")\n",
    "\n",
    "\n",
    "  # Cross Entropy Loss \n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "  ########################\n",
    "  ######## Dataset #######\n",
    "  ########################\n",
    "  train_dataset = TextDataset(\n",
    "    dataframe=train_df,\n",
    "    max_len=train_df[\"text\"].str.split().apply(len).max(),\n",
    "    embedding_matrix_vocab_to_index=embedding_matrix_vocab_to_index_train\n",
    "  )\n",
    "  train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "  val_dataset = TextDataset(\n",
    "    dataframe=val_df,\n",
    "    max_len=train_df[\"text\"].str.split().apply(len).max(),\n",
    "    embedding_matrix_vocab_to_index=embedding_matrix_vocab_to_index_train\n",
    "  )\n",
    "  val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "  ########################\n",
    "  ######### Train ########\n",
    "  ########################\n",
    "  model, avg_train_loss, avg_train_acc, avg_val_loss, avg_val_acc, num_of_epochs = train(\n",
    "    model=model_rnn,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    min_epoch=min_epoch,\n",
    "    max_epoch=max_epochs,\n",
    "    show_progress=show_progress\n",
    "  )\n",
    "\n",
    "  ########################\n",
    "  ######### Plot #########\n",
    "  ########################\n",
    "  subtitle = f\"batch_size_{batch_size}; lr_{learning_rate}; optimizer_{optimizer_name}; hidden_dim_{hidden_dim}; num_layers_{num_layers}\"\n",
    "  save_filename_prefix = f\"rnn/last/batch_size_{batch_size}-lr_{learning_rate}-optimizer_{optimizer_name}-hidden_dim_{hidden_dim}-num_layers_{num_layers}\"\n",
    "  \n",
    "  # Plot Train Loss and Accuracy Graph\n",
    "  plot_loss_acc_graph(\n",
    "    loss_list=avg_train_loss, \n",
    "    acc_list=avg_train_acc, \n",
    "    dataset_type=\"train\",\n",
    "    subtitle=subtitle,\n",
    "    save_filename_prefix=save_filename_prefix,\n",
    "    display=False\n",
    "  )\n",
    "\n",
    "  # Plot Validation Loss and Accuracy Graph\n",
    "  plot_loss_acc_graph(\n",
    "    loss_list=avg_val_loss, \n",
    "    acc_list=avg_val_acc, \n",
    "    dataset_type=\"val\",\n",
    "    subtitle=subtitle,\n",
    "    save_filename_prefix=save_filename_prefix,\n",
    "    display=False\n",
    "  )\n",
    "\n",
    "  ########################\n",
    "  ##### Return Value #####\n",
    "  ########################\n",
    "  configuration_results = {\n",
    "    \"model_id\": None, # To keep track of trained model object\n",
    "\n",
    "    \"batch_size\": batch_size,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"optimizer_name\": optimizer_name,\n",
    "\n",
    "    # RNN Model Parameters\n",
    "    \"hidden_dim\": hidden_dim,\n",
    "    \"num_layers\": num_layers,\n",
    "\n",
    "    # Model performance\n",
    "    \"train_loss\": avg_train_loss[-1],\n",
    "    \"train_accuracy\": avg_train_acc[-1],\n",
    "    \"val_loss\": avg_val_loss[-1],\n",
    "    \"val_accuracy\": avg_val_acc[-1],\n",
    "\n",
    "    # Epoch Number\n",
    "    \"num_of_epochs\": num_of_epochs\n",
    "  }\n",
    "  return model, configuration_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training] ---------- batch_size_32; lr_0.001; optimizer_SGD; hidden_dim_16; num_layers_2 ----------[Training] ---------- batch_size_32; lr_0.01; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n",
      "[Training] ---------- batch_size_32; lr_0.05; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n",
      "[Training] ---------- batch_size_32; lr_0.1; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n",
      "[Training] ---------- batch_size_64; lr_0.001; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n",
      "[Training] ---------- batch_size_64; lr_0.01; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n",
      "\n",
      "[Training] ---------- batch_size_64; lr_0.05; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n",
      "[Training] ---------- batch_size_64; lr_0.1; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n",
      "[Done] ---------- batch_size_64; lr_0.1; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n",
      "[Training] ---------- batch_size_128; lr_0.001; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n",
      "[Done] ---------- batch_size_32; lr_0.1; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n",
      "[Training] ---------- batch_size_128; lr_0.01; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n",
      "[Done] ---------- batch_size_32; lr_0.01; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n",
      "[Training] ---------- batch_size_128; lr_0.05; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n",
      "[Done] ---------- batch_size_64; lr_0.05; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n",
      "[Training] ---------- batch_size_128; lr_0.1; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n",
      "[Done] ---------- batch_size_64; lr_0.01; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n",
      "[Training] ---------- batch_size_256; lr_0.001; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n",
      "[Done] ---------- batch_size_32; lr_0.05; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n",
      "[Training] ---------- batch_size_256; lr_0.01; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/yuri/sc4002-nlp-sentiment-classification/part2.ipynb Cell 8\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yuri/sc4002-nlp-sentiment-classification/part2.ipynb#X10sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m           q\u001b[39m.\u001b[39mput({\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yuri/sc4002-nlp-sentiment-classification/part2.ipynb#X10sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mmodel_id\u001b[39m\u001b[39m\"\u001b[39m: current_model_id,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yuri/sc4002-nlp-sentiment-classification/part2.ipynb#X10sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m\"\u001b[39m: batch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yuri/sc4002-nlp-sentiment-classification/part2.ipynb#X10sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnum_layers\u001b[39m\u001b[39m\"\u001b[39m: num_layers\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yuri/sc4002-nlp-sentiment-classification/part2.ipynb#X10sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m           })\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yuri/sc4002-nlp-sentiment-classification/part2.ipynb#X10sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39m# Wait for queue to be empty\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/yuri/sc4002-nlp-sentiment-classification/part2.ipynb#X10sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m q\u001b[39m.\u001b[39;49mjoin()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yuri/sc4002-nlp-sentiment-classification/part2.ipynb#X10sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39m# Stop workers by passing None to each thread\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yuri/sc4002-nlp-sentiment-classification/part2.ipynb#X10sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_threads):\n",
      "File \u001b[0;32m~/miniconda3/envs/habitat/lib/python3.9/queue.py:90\u001b[0m, in \u001b[0;36mQueue.join\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_tasks_done:\n\u001b[1;32m     89\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munfinished_tasks:\n\u001b[0;32m---> 90\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mall_tasks_done\u001b[39m.\u001b[39;49mwait()\n",
      "File \u001b[0;32m~/miniconda3/envs/habitat/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Done] ---------- batch_size_128; lr_0.05; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n",
      "[Training] ---------- batch_size_256; lr_0.05; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n",
      "[Done] ---------- batch_size_128; lr_0.001; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n",
      "[Training] ---------- batch_size_256; lr_0.1; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n",
      "[Done] ---------- batch_size_128; lr_0.1; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n",
      "[Training] ---------- batch_size_32; lr_0.001; optimizer_Adagrad; hidden_dim_16; num_layers_2 ----------\n",
      "[Done] ---------- batch_size_32; lr_0.001; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n",
      "[Training] ---------- batch_size_32; lr_0.01; optimizer_Adagrad; hidden_dim_16; num_layers_2 ----------\n",
      "[Done] ---------- batch_size_256; lr_0.05; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n",
      "[Training] ---------- batch_size_32; lr_0.05; optimizer_Adagrad; hidden_dim_16; num_layers_2 ----------\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "############ With Multiprocessing ###########\n",
    "#############################################\n",
    "\n",
    "from queue import Queue\n",
    "import threading\n",
    "\n",
    "all_configurations_results = []\n",
    "rnn_models = {}\n",
    "\n",
    "# Function to call\n",
    "def execute_thread(q):\n",
    "  while True:\n",
    "    config = q.get()\n",
    "\n",
    "    if config is None:  # Break the loop if a 'None' is passed to signal the end of the work\n",
    "      break\n",
    "\n",
    "    model_id = config[\"model_id\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    learning_rate = config[\"learning_rate\"]\n",
    "    optimizer_name = config[\"optimizer_name\"]\n",
    "    hidden_dim = config[\"hidden_dim\"]\n",
    "    num_layers = config[\"num_layers\"]\n",
    "\n",
    "    print(f\"[Training] ---------- batch_size_{batch_size}; lr_{learning_rate}; optimizer_{optimizer_name}; hidden_dim_{hidden_dim}; num_layers_{num_layers} ----------\")\n",
    "    \n",
    "    model, configuration_results = train_rnn_model_with_parameters(\n",
    "      batch_size=config[\"batch_size\"],\n",
    "      learning_rate=config[\"learning_rate\"],\n",
    "      optimizer_name=config[\"optimizer_name\"],\n",
    "      hidden_dim=config[\"hidden_dim\"],\n",
    "      num_layers=config[\"num_layers\"],\n",
    "      show_progress=False\n",
    "    )\n",
    "    configuration_results[\"model_id\"] = model_id\n",
    "\n",
    "    with threading.Lock():\n",
    "        all_configurations_results.append(configuration_results)\n",
    "        rnn_models[model_id] = model\n",
    "    \n",
    "    print(f\"[Done] ---------- batch_size_{batch_size}; lr_{learning_rate}; optimizer_{optimizer_name}; hidden_dim_{hidden_dim}; num_layers_{num_layers} ----------\")\n",
    "\n",
    "    q.task_done()\n",
    "\n",
    "q = Queue(maxsize=0) # Infinite Size Queue\n",
    "num_threads = 8 # Number of threads\n",
    "\n",
    "# Create threads and run them in background\n",
    "threads = []\n",
    "for i in range(num_threads):\n",
    "  worker = threading.Thread(target=execute_thread, args=(q,), daemon=True)\n",
    "  worker.start()\n",
    "  threads.append(worker)\n",
    "\n",
    "# Populate Queue\n",
    "current_model_id = 0\n",
    "for hidden_dim in SEARCH_SPACE[\"hidden_dim\"]:\n",
    "  for num_layers in SEARCH_SPACE[\"num_layers\"]:\n",
    "    for optimizer_name in SEARCH_SPACE[\"optimizer_name\"]:\n",
    "      for batch_size in SEARCH_SPACE[\"batch_size\"]:\n",
    "        for learning_rate in SEARCH_SPACE[\"learning_rate\"]:\n",
    "          current_model_id += 1\n",
    "          q.put({\n",
    "            \"model_id\": current_model_id,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"optimizer_name\": optimizer_name,\n",
    "            \"hidden_dim\": hidden_dim,\n",
    "            \"num_layers\": num_layers\n",
    "          })\n",
    "\n",
    "# Wait for queue to be empty\n",
    "q.join()\n",
    "\n",
    "# Stop workers by passing None to each thread\n",
    "for _ in range(num_threads):\n",
    "  q.put(None)\n",
    "\n",
    "# Wait for all threads to finish\n",
    "for thread in threads:\n",
    "  thread.join()\n",
    "\n",
    "print(\"All tasks completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- batch_size_32; lr_0.001; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Train): 100%|██████████| 267/267 [00:05<00:00, 50.38it/s, acc=0.5, loss=0.694]  \n",
      "Epoch 1 (Val): 100%|██████████| 34/34 [00:00<00:00, 103.04it/s, acc=0.5, loss=0.694]  \n",
      "Epoch 2 (Train): 100%|██████████| 267/267 [00:05<00:00, 53.04it/s, acc=0.5, loss=0.694]  \n",
      "Epoch 2 (Val): 100%|██████████| 34/34 [00:00<00:00, 98.09it/s, acc=0.5, loss=0.694]  \n",
      "Epoch 3 (Train): 100%|██████████| 267/267 [00:05<00:00, 52.48it/s, acc=0.5, loss=0.694]  \n",
      "Epoch 3 (Val): 100%|██████████| 34/34 [00:00<00:00, 99.73it/s, acc=0.502, loss=0.693] \n",
      "Epoch 4 (Train):   6%|▌         | 16/267 [00:00<00:04, 51.29it/s, acc=0.52, loss=0.692] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m current_model_id \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---------- batch_size_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; lr_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; optimizer_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimizer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; hidden_dim_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; num_layers_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ----------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m model, configuration_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_rnn_model_with_parameters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m  \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m  \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m  \u001b[49m\u001b[43moptimizer_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m  \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m  \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m  \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     25\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m configuration_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m current_model_id\n\u001b[0;32m     28\u001b[0m all_configurations_results\u001b[38;5;241m.\u001b[39mappend(configuration_results)\n",
      "Cell \u001b[1;32mIn[4], line 69\u001b[0m, in \u001b[0;36mtrain_rnn_model_with_parameters\u001b[1;34m(batch_size, learning_rate, optimizer_name, hidden_dim, num_layers, show_progress)\u001b[0m\n\u001b[0;32m     64\u001b[0m val_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m########################\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m######### Train ########\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m########################\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m model, avg_train_loss, avg_train_acc, avg_val_loss, avg_val_acc, num_of_epochs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_rnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m  \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m  \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m  \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmin_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmax_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m  \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m########################\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m######### Plot #########\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m########################\u001b[39;00m\n\u001b[0;32m     83\u001b[0m subtitle \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; lr_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; optimizer_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimizer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; hidden_dim_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; num_layers_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Toh Jing Qiang\\Desktop\\sc4002-nlp-sentiment-classification\\solver.py:43\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, criterion, optimizer, train_dataloader, val_dataloader, min_epoch, max_epoch, max_non_increasing_epoch_count, show_progress)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     41\u001b[0m     num_of_epochs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 43\u001b[0m     epoch_train_loss, epoch_train_acc \u001b[38;5;241m=\u001b[39m \u001b[43m__train_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m     epoch_val_loss, epoch_val_acc \u001b[38;5;241m=\u001b[39m __validate(model, criterion, val_dataloader, epoch, show_progress)\n\u001b[0;32m     46\u001b[0m     avg_train_loss\u001b[38;5;241m.\u001b[39mappend(epoch_train_loss)\n",
      "File \u001b[1;32mc:\\Users\\Toh Jing Qiang\\Desktop\\sc4002-nlp-sentiment-classification\\solver.py:85\u001b[0m, in \u001b[0;36m__train_one_epoch\u001b[1;34m(model, train_dataloader, optimizer, criterion, epoch, show_progress)\u001b[0m\n\u001b[0;32m     82\u001b[0m     t\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m (Train)\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m epoch)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Mini-Batch\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Toh Jing Qiang\\Desktop\\sc4002-nlp-sentiment-classification\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Toh Jing Qiang\\Desktop\\sc4002-nlp-sentiment-classification\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Toh Jing Qiang\\Desktop\\sc4002-nlp-sentiment-classification\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Toh Jing Qiang\\Desktop\\sc4002-nlp-sentiment-classification\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Toh Jing Qiang\\Desktop\\sc4002-nlp-sentiment-classification\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:211\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransposed\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Toh Jing Qiang\\Desktop\\sc4002-nlp-sentiment-classification\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Toh Jing Qiang\\Desktop\\sc4002-nlp-sentiment-classification\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\Toh Jing Qiang\\Desktop\\sc4002-nlp-sentiment-classification\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 (Train):   6%|▋         | 17/267 [00:13<00:04, 51.29it/s, acc=0.52, loss=0.692]"
     ]
    }
   ],
   "source": [
    "# ############################################\n",
    "# ######### Without Multiprocessing ##########\n",
    "# ############################################\n",
    "\n",
    "# all_configurations_results = []\n",
    "# rnn_models = {}\n",
    "\n",
    "# current_model_id = 0\n",
    "\n",
    "# for hidden_dim in SEARCH_SPACE[\"hidden_dim\"]:\n",
    "#   for num_layers in SEARCH_SPACE[\"num_layers\"]:\n",
    "#     for optimizer_name in SEARCH_SPACE[\"optimizer_name\"]:\n",
    "#       for batch_size in SEARCH_SPACE[\"batch_size\"]:\n",
    "#         for learning_rate in SEARCH_SPACE[\"learning_rate\"]:\n",
    "#           current_model_id += 1\n",
    "\n",
    "#           print(f\"---------- batch_size_{batch_size}; lr_{learning_rate}; optimizer_{optimizer_name}; hidden_dim_{hidden_dim}; num_layers_{num_layers} ----------\")\n",
    "#           model, configuration_results = train_rnn_model_with_parameters(\n",
    "#             batch_size=batch_size,\n",
    "#             learning_rate=learning_rate,\n",
    "#             optimizer_name=optimizer_name,\n",
    "#             hidden_dim=hidden_dim,\n",
    "#             num_layers=num_layers,\n",
    "#             show_progress=True\n",
    "#           )\n",
    "#           configuration_results[\"model_id\"] = current_model_id\n",
    "\n",
    "#           all_configurations_results.append(configuration_results)\n",
    "#           rnn_models[current_model_id] = model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Configurations Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>optimizer_name</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>num_of_epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>0.010</td>\n",
       "      <td>SGD</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.693316</td>\n",
       "      <td>0.491989</td>\n",
       "      <td>0.693116</td>\n",
       "      <td>0.504044</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>0.100</td>\n",
       "      <td>SGD</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.693710</td>\n",
       "      <td>0.502575</td>\n",
       "      <td>0.693118</td>\n",
       "      <td>0.504044</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.050</td>\n",
       "      <td>SGD</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.693481</td>\n",
       "      <td>0.495409</td>\n",
       "      <td>0.693155</td>\n",
       "      <td>0.504044</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>0.001</td>\n",
       "      <td>SGD</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.693168</td>\n",
       "      <td>0.490481</td>\n",
       "      <td>0.693146</td>\n",
       "      <td>0.502022</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_id  batch_size  learning_rate optimizer_name  hidden_dim  num_layers  \\\n",
       "0         2          32          0.010            SGD          16           2   \n",
       "1         4          32          0.100            SGD          16           2   \n",
       "2         3          32          0.050            SGD          16           2   \n",
       "3         1          32          0.001            SGD          16           2   \n",
       "\n",
       "   train_loss  train_accuracy  val_loss  val_accuracy  num_of_epochs  \n",
       "0    0.693316        0.491989  0.693116      0.504044             21  \n",
       "1    0.693710        0.502575  0.693118      0.504044             22  \n",
       "2    0.693481        0.495409  0.693155      0.504044             36  \n",
       "3    0.693168        0.490481  0.693146      0.502022             23  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_configurations_results_df = pd.DataFrame.from_dict(all_configurations_results)\n",
    "model_configurations_results_df.sort_values(by=[\"val_accuracy\"], ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a. Final Configuration of best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>optimizer_name</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>num_of_epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>0.01</td>\n",
       "      <td>SGD</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.693316</td>\n",
       "      <td>0.491989</td>\n",
       "      <td>0.693116</td>\n",
       "      <td>0.504044</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_id  batch_size  learning_rate optimizer_name  hidden_dim  num_layers  \\\n",
       "0         2          32           0.01            SGD          16           2   \n",
       "\n",
       "   train_loss  train_accuracy  val_loss  val_accuracy  num_of_epochs  \n",
       "0    0.693316        0.491989  0.693116      0.504044             21  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rnn_model_configuration = model_configurations_results_df.head(1)\n",
    "best_rnn_model_configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(16332, 300)\n",
       "  (rnn): RNN(300, 16, num_layers=2, batch_first=True)\n",
       "  (relu): ReLU()\n",
       "  (fc): Linear(in_features=16, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rnn_model_id = best_rnn_model_configuration[\"model_id\"][0]\n",
    "best_rnn_model = rnn_models[best_rnn_model_id]\n",
    "best_rnn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export Best Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>optimizer_name</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>num_of_epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>0.001</td>\n",
       "      <td>SGD</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.693179</td>\n",
       "      <td>0.495552</td>\n",
       "      <td>0.693145</td>\n",
       "      <td>0.502022</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_id  batch_size  learning_rate optimizer_name  hidden_dim  num_layers  \\\n",
       "0         1          32          0.001            SGD          16           2   \n",
       "\n",
       "   train_loss  train_accuracy  val_loss  val_accuracy  num_of_epochs  \n",
       "0    0.693179        0.495552  0.693145      0.502022             29  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rnn_model_configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving object to local...\n",
      "Object saved to local!\n"
     ]
    }
   ],
   "source": [
    "from utils.file import save_to_local_file\n",
    "\n",
    "model_name = f\"batch_size_{batch_size}-lr_{learning_rate}-optimizer_{optimizer_name}-hidden_dim_{hidden_dim}-num_layers_{num_layers}\"\n",
    "save_to_local_file(f\"models/rnn/{model_name}.pckl\", best_rnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b. Accuracy on Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6931904\n",
      "Test Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "from solver import test\n",
    "\n",
    "########################\n",
    "######## Dataset #######\n",
    "########################\n",
    "test_dataset = TextDataset(\n",
    "  dataframe=test_df,\n",
    "  max_len=test_df[\"text\"].str.split().apply(len).max(),\n",
    "  embedding_matrix_vocab_to_index=embedding_matrix_vocab_to_index_train\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False)\n",
    "\n",
    "########################\n",
    "######### Train ########\n",
    "########################\n",
    "test_loss, test_accuracy = test(\n",
    "  model=best_rnn_model,\n",
    "  criterion=nn.CrossEntropyLoss(),\n",
    "  test_dataloader=test_dataloader,\n",
    ")\n",
    "\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c. Strategies to derive final sentence representation\n",
    "\n",
    "1. Last State: Use last hidden state as sentence representation\n",
    "2. Max Pooling\n",
    "3. Average Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
