{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Embeddings and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading object from local...\n",
      "Object loaded from local!\n",
      "Loading object from local...\n",
      "Object loaded from local!\n"
     ]
    }
   ],
   "source": [
    "# Import Embedding Matrix and Embedding Matrix's Train dataset vocab to index dictionary\n",
    "from utils.file import load_from_local_file\n",
    "\n",
    "embedding_matrix = load_from_local_file(\"models/embedding_matrix.pckl\")\n",
    "embedding_matrix_train_dataset_vocab_to_index: dict = load_from_local_file(\"models/embedding_matrix_train_dataset_vocab_to_index.pckl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the rock is destined to be the 21st century's ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the gorgeously elaborate continuation of \" the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>effective but too-tepid biopic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if you sometimes like to go to the movies to h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>emerges as something rare , an issue movie tha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  the rock is destined to be the 21st century's ...      1\n",
       "1  the gorgeously elaborate continuation of \" the...      1\n",
       "2                     effective but too-tepid biopic      1\n",
       "3  if you sometimes like to go to the movies to h...      1\n",
       "4  emerges as something rare , an issue movie tha...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"datasets/train.csv\")\n",
    "val_df = pd.read_csv(\"datasets/val.csv\")\n",
    "test_df = pd.read_csv(\"datasets/test.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_SPACE = {\n",
    "  \"batch_size\": [32, 64, 128, 256],\n",
    "  \"learning_rate\": [0.001, 0.01, 0.05, 0.1],\n",
    "  \"optimizer_name\": [\"SGD\", \"Adagrad\", \"Adam\", \"RMSprop\"],\n",
    "\n",
    "  # RNN Model Parameters\n",
    "  \"hidden_dim\": [16],\n",
    "  \"num_layers\": [2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Toh Jing\n",
      "[nltk_data]     Qiang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to C:\\Users\\Toh Jing\n",
      "[nltk_data]     Qiang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Toh Jing\n",
      "[nltk_data]     Qiang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from models.RNN import RNN\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from solver import train, plot_loss_acc_graph\n",
    "from utils.custom_dataset import TextDataset\n",
    "\n",
    "def train_rnn_model_with_parameters(\n",
    "    batch_size: int,\n",
    "    learning_rate: float,\n",
    "    optimizer_name: str,\n",
    "    hidden_dim: int,\n",
    "    num_layers: int,\n",
    "):\n",
    "  # Model\n",
    "  model_rnn = RNN(\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    output_dim=2,\n",
    "    sentence_representation_type=\"last\"\n",
    "  )\n",
    "\n",
    "  ########################\n",
    "  ###### Parameters ######\n",
    "  ########################\n",
    "  batch_size = batch_size\n",
    "  min_epoch = 20\n",
    "  max_epochs = 10_000\n",
    "\n",
    "  # SGD Optimizer\n",
    "  learning_rate = learning_rate\n",
    "  match optimizer_name:\n",
    "    case \"SGD\":\n",
    "      optimizer = torch.optim.SGD(model_rnn.parameters(), lr=learning_rate)\n",
    "    case \"Adagrad\":\n",
    "      optimizer = torch.optim.Adagrad(model_rnn.parameters(), lr=learning_rate)\n",
    "    case \"Adam\":\n",
    "      optimizer = torch.optim.Adam(model_rnn.parameters(), lr=learning_rate)\n",
    "    case \"RMSprop\":\n",
    "      optimizer = torch.optim.RMSprop(model_rnn.parameters(), lr=learning_rate)\n",
    "    case _:\n",
    "      raise Exception(\"Invalid optimizer name!\")\n",
    "\n",
    "  # Cross Entropy Loss \n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "  ########################\n",
    "  ######## Dataset #######\n",
    "  ########################\n",
    "  train_dataset = TextDataset(\n",
    "    dataframe=train_df,\n",
    "    max_len=train_df[\"text\"].str.split().apply(len).max(),\n",
    "    embedding_matrix_vocab_to_index=embedding_matrix_train_dataset_vocab_to_index\n",
    "  )\n",
    "  train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "  val_dataset = TextDataset(\n",
    "    dataframe=val_df,\n",
    "    max_len=train_df[\"text\"].str.split().apply(len).max(),\n",
    "    embedding_matrix_vocab_to_index=embedding_matrix_train_dataset_vocab_to_index\n",
    "  )\n",
    "  val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "  ########################\n",
    "  ######### Train ########\n",
    "  ########################\n",
    "  model, avg_train_loss, avg_train_acc, avg_val_loss, avg_val_acc, num_of_epochs = train(\n",
    "    model=model_rnn,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    min_epoch=min_epoch,\n",
    "    max_epoch=max_epochs,\n",
    "  )\n",
    "\n",
    "  ########################\n",
    "  ######### Plot #########\n",
    "  ########################\n",
    "  subtitle = f\"batch_size_{batch_size}; lr_{learning_rate}; optimizer_{optimizer_name}; hidden_dim_{hidden_dim}; num_layers_{num_layers}\"\n",
    "  save_filename_prefix = f\"rnn/last/batch_size_{batch_size}-lr_{learning_rate}-optimizer_{optimizer_name}-hidden_dim_{hidden_dim}-num_layers_{num_layers}\"\n",
    "  \n",
    "  # Plot Train Loss and Accuracy Graph\n",
    "  plot_loss_acc_graph(\n",
    "    loss_list=avg_train_loss, \n",
    "    acc_list=avg_train_acc, \n",
    "    dataset_type=\"train\",\n",
    "    subtitle=subtitle,\n",
    "    save_filename_prefix=save_filename_prefix,\n",
    "    display=False\n",
    "  )\n",
    "\n",
    "  # Plot Validation Loss and Accuracy Graph\n",
    "  plot_loss_acc_graph(\n",
    "    loss_list=avg_val_loss, \n",
    "    acc_list=avg_val_acc, \n",
    "    dataset_type=\"val\",\n",
    "    subtitle=subtitle,\n",
    "    save_filename_prefix=save_filename_prefix,\n",
    "    display=False\n",
    "  )\n",
    "\n",
    "  ########################\n",
    "  ##### Return Value #####\n",
    "  ########################\n",
    "  configuration_results = {\n",
    "    \"model_id\": None, # To keep track of trained model object\n",
    "\n",
    "    \"batch_size\": batch_size,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"optimizer_name\": optimizer_name,\n",
    "\n",
    "    # RNN Model Parameters\n",
    "    \"hidden_dim\": hidden_dim,\n",
    "    \"num_layers\": num_layers,\n",
    "\n",
    "    # Model performance\n",
    "    \"train_loss\": avg_train_loss[-1],\n",
    "    \"train_accuracy\": avg_train_acc[-1],\n",
    "    \"val_loss\": avg_val_loss[-1],\n",
    "    \"val_accuracy\": avg_val_acc[-1],\n",
    "\n",
    "    # Epoch Number\n",
    "    \"num_of_epochs\": num_of_epochs\n",
    "  }\n",
    "  return model, configuration_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- batch_size_32; lr_0.001; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Train): 100%|██████████| 267/267 [00:09<00:00, 28.49it/s, acc=0.5, loss=0.693]  \n",
      "Epoch 1 (Val): 100%|██████████| 34/34 [00:00<00:00, 60.30it/s, acc=0.496, loss=0.693]\n",
      "Epoch 2 (Train): 100%|██████████| 267/267 [00:08<00:00, 30.44it/s, acc=0.5, loss=0.693]  \n",
      "Epoch 2 (Val): 100%|██████████| 34/34 [00:00<00:00, 58.84it/s, acc=0.496, loss=0.693]\n",
      "Epoch 3 (Train): 100%|██████████| 267/267 [00:08<00:00, 30.14it/s, acc=0.5, loss=0.693]  \n",
      "Epoch 3 (Val): 100%|██████████| 34/34 [00:00<00:00, 55.21it/s, acc=0.498, loss=0.693]\n",
      "Epoch 4 (Train): 100%|██████████| 267/267 [00:09<00:00, 27.60it/s, acc=0.5, loss=0.693]  \n",
      "Epoch 4 (Val): 100%|██████████| 34/34 [00:00<00:00, 55.19it/s, acc=0.506, loss=0.693]\n",
      "Epoch 5 (Train): 100%|██████████| 267/267 [00:10<00:00, 26.18it/s, acc=0.5, loss=0.693]  \n",
      "Epoch 5 (Val): 100%|██████████| 34/34 [00:00<00:00, 49.08it/s, acc=0.498, loss=0.693]\n",
      "Epoch 6 (Train): 100%|██████████| 267/267 [00:10<00:00, 24.53it/s, acc=0.491, loss=0.693]\n",
      "Epoch 6 (Val): 100%|██████████| 34/34 [00:00<00:00, 40.89it/s, acc=0.496, loss=0.693]\n",
      "Epoch 7 (Train): 100%|██████████| 267/267 [00:10<00:00, 24.63it/s, acc=0.498, loss=0.693]\n",
      "Epoch 7 (Val): 100%|██████████| 34/34 [00:00<00:00, 53.92it/s, acc=0.496, loss=0.693]\n",
      "Epoch 8 (Train): 100%|██████████| 267/267 [00:08<00:00, 31.06it/s, acc=0.485, loss=0.693]\n",
      "Epoch 8 (Val): 100%|██████████| 34/34 [00:00<00:00, 64.00it/s, acc=0.5, loss=0.693]  \n",
      "Epoch 9 (Train): 100%|██████████| 267/267 [00:11<00:00, 22.64it/s, acc=0.495, loss=0.693]\n",
      "Epoch 9 (Val): 100%|██████████| 34/34 [00:00<00:00, 40.32it/s, acc=0.502, loss=0.693]\n",
      "Epoch 10 (Train): 100%|██████████| 267/267 [00:11<00:00, 24.12it/s, acc=0.495, loss=0.693]\n",
      "Epoch 10 (Val): 100%|██████████| 34/34 [00:00<00:00, 50.84it/s, acc=0.5, loss=0.693]  \n",
      "Epoch 11 (Train): 100%|██████████| 267/267 [00:10<00:00, 26.05it/s, acc=0.489, loss=0.693]\n",
      "Epoch 11 (Val): 100%|██████████| 34/34 [00:00<00:00, 52.29it/s, acc=0.502, loss=0.693]\n",
      "Epoch 12 (Train): 100%|██████████| 267/267 [00:09<00:00, 27.57it/s, acc=0.5, loss=0.693]  \n",
      "Epoch 12 (Val): 100%|██████████| 34/34 [00:00<00:00, 58.02it/s, acc=0.494, loss=0.693]\n",
      "Epoch 13 (Train): 100%|██████████| 267/267 [00:09<00:00, 27.81it/s, acc=0.49, loss=0.693] \n",
      "Epoch 13 (Val): 100%|██████████| 34/34 [00:00<00:00, 61.70it/s, acc=0.5, loss=0.693]  \n",
      "Epoch 14 (Train): 100%|██████████| 267/267 [00:08<00:00, 30.59it/s, acc=0.499, loss=0.693]\n",
      "Epoch 14 (Val): 100%|██████████| 34/34 [00:00<00:00, 57.06it/s, acc=0.5, loss=0.693]  \n",
      "Epoch 15 (Train): 100%|██████████| 267/267 [00:09<00:00, 29.06it/s, acc=0.496, loss=0.693]\n",
      "Epoch 15 (Val): 100%|██████████| 34/34 [00:00<00:00, 58.81it/s, acc=0.51, loss=0.693] \n",
      "Epoch 16 (Train): 100%|██████████| 267/267 [00:08<00:00, 29.98it/s, acc=0.493, loss=0.693]\n",
      "Epoch 16 (Val): 100%|██████████| 34/34 [00:00<00:00, 61.64it/s, acc=0.5, loss=0.693]  \n",
      "Epoch 17 (Train): 100%|██████████| 267/267 [00:08<00:00, 31.08it/s, acc=0.492, loss=0.693]\n",
      "Epoch 17 (Val): 100%|██████████| 34/34 [00:00<00:00, 59.38it/s, acc=0.5, loss=0.693]  \n",
      "Epoch 18 (Train): 100%|██████████| 267/267 [00:08<00:00, 29.97it/s, acc=0.494, loss=0.693]\n",
      "Epoch 18 (Val): 100%|██████████| 34/34 [00:00<00:00, 60.82it/s, acc=0.502, loss=0.693]\n",
      "Epoch 19 (Train): 100%|██████████| 267/267 [00:08<00:00, 30.88it/s, acc=0.493, loss=0.693]\n",
      "Epoch 19 (Val): 100%|██████████| 34/34 [00:00<00:00, 55.50it/s, acc=0.502, loss=0.693]\n",
      "Epoch 20 (Train): 100%|██████████| 267/267 [00:14<00:00, 18.42it/s, acc=0.498, loss=0.693]\n",
      "Epoch 20 (Val): 100%|██████████| 34/34 [00:00<00:00, 47.72it/s, acc=0.5, loss=0.693]  \n",
      "Epoch 21 (Train): 100%|██████████| 267/267 [00:09<00:00, 26.76it/s, acc=0.491, loss=0.693]\n",
      "Epoch 21 (Val): 100%|██████████| 34/34 [00:00<00:00, 48.92it/s, acc=0.492, loss=0.693]\n",
      "Epoch 22 (Train): 100%|██████████| 267/267 [00:09<00:00, 26.88it/s, acc=0.494, loss=0.693]\n",
      "Epoch 22 (Val): 100%|██████████| 34/34 [00:00<00:00, 48.45it/s, acc=0.502, loss=0.693]\n",
      "Epoch 23 (Train): 100%|██████████| 267/267 [00:10<00:00, 25.63it/s, acc=0.495, loss=0.693]\n",
      "Epoch 23 (Val): 100%|██████████| 34/34 [00:00<00:00, 59.25it/s, acc=0.492, loss=0.693]\n",
      "Epoch 24 (Train): 100%|██████████| 267/267 [00:11<00:00, 23.91it/s, acc=0.499, loss=0.693]\n",
      "Epoch 24 (Val): 100%|██████████| 34/34 [00:00<00:00, 40.58it/s, acc=0.502, loss=0.693]\n",
      "Epoch 25 (Train): 100%|██████████| 267/267 [00:09<00:00, 28.49it/s, acc=0.491, loss=0.693]\n",
      "Epoch 25 (Val): 100%|██████████| 34/34 [00:00<00:00, 57.85it/s, acc=0.502, loss=0.693]\n",
      "Epoch 26 (Train): 100%|██████████| 267/267 [00:09<00:00, 29.39it/s, acc=0.501, loss=0.693]\n",
      "Epoch 26 (Val): 100%|██████████| 34/34 [00:00<00:00, 58.79it/s, acc=0.502, loss=0.693]\n",
      "Epoch 27 (Train): 100%|██████████| 267/267 [00:08<00:00, 30.03it/s, acc=0.494, loss=0.693]\n",
      "Epoch 27 (Val): 100%|██████████| 34/34 [00:00<00:00, 59.08it/s, acc=0.5, loss=0.693]  \n",
      "Epoch 28 (Train): 100%|██████████| 267/267 [00:09<00:00, 27.47it/s, acc=0.494, loss=0.693]\n",
      "Epoch 28 (Val): 100%|██████████| 34/34 [00:00<00:00, 40.09it/s, acc=0.5, loss=0.693]  \n",
      "Epoch 29 (Train): 100%|██████████| 267/267 [00:14<00:00, 18.61it/s, acc=0.496, loss=0.693]\n",
      "Epoch 29 (Val): 100%|██████████| 34/34 [00:00<00:00, 35.60it/s, acc=0.502, loss=0.693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- batch_size_32; lr_0.01; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Train): 100%|██████████| 267/267 [00:09<00:00, 28.22it/s, acc=0.494, loss=0.693]\n",
      "Epoch 1 (Val): 100%|██████████| 34/34 [00:00<00:00, 54.57it/s, acc=0.502, loss=0.693]\n",
      "Epoch 2 (Train): 100%|██████████| 267/267 [00:08<00:00, 30.09it/s, acc=0.498, loss=0.693]\n",
      "Epoch 2 (Val): 100%|██████████| 34/34 [00:00<00:00, 69.97it/s, acc=0.496, loss=0.693]\n",
      "Epoch 3 (Train): 100%|██████████| 267/267 [00:07<00:00, 33.54it/s, acc=0.49, loss=0.693] \n",
      "Epoch 3 (Val): 100%|██████████| 34/34 [00:00<00:00, 55.56it/s, acc=0.504, loss=0.693]\n",
      "Epoch 4 (Train): 100%|██████████| 267/267 [00:08<00:00, 30.32it/s, acc=0.502, loss=0.693]\n",
      "Epoch 4 (Val): 100%|██████████| 34/34 [00:00<00:00, 64.78it/s, acc=0.496, loss=0.693]\n",
      "Epoch 5 (Train): 100%|██████████| 267/267 [00:08<00:00, 32.85it/s, acc=0.493, loss=0.693]\n",
      "Epoch 5 (Val): 100%|██████████| 34/34 [00:00<00:00, 37.59it/s, acc=0.496, loss=0.693]\n",
      "Epoch 6 (Train): 100%|██████████| 267/267 [00:09<00:00, 28.64it/s, acc=0.499, loss=0.693]\n",
      "Epoch 6 (Val): 100%|██████████| 34/34 [00:00<00:00, 49.45it/s, acc=0.502, loss=0.693]\n",
      "Epoch 7 (Train): 100%|██████████| 267/267 [00:09<00:00, 28.83it/s, acc=0.496, loss=0.693]\n",
      "Epoch 7 (Val): 100%|██████████| 34/34 [00:00<00:00, 65.95it/s, acc=0.5, loss=0.693]  \n",
      "Epoch 8 (Train): 100%|██████████| 267/267 [00:08<00:00, 31.28it/s, acc=0.497, loss=0.693]\n",
      "Epoch 8 (Val): 100%|██████████| 34/34 [00:00<00:00, 55.46it/s, acc=0.494, loss=0.693]\n",
      "Epoch 9 (Train): 100%|██████████| 267/267 [00:07<00:00, 35.77it/s, acc=0.488, loss=0.693]\n",
      "Epoch 9 (Val): 100%|██████████| 34/34 [00:00<00:00, 74.63it/s, acc=0.502, loss=0.693]\n",
      "Epoch 10 (Train): 100%|██████████| 267/267 [00:07<00:00, 33.52it/s, acc=0.498, loss=0.693]\n",
      "Epoch 10 (Val): 100%|██████████| 34/34 [00:00<00:00, 52.90it/s, acc=0.502, loss=0.693]\n",
      "Epoch 11 (Train): 100%|██████████| 267/267 [00:08<00:00, 31.81it/s, acc=0.492, loss=0.693]\n",
      "Epoch 11 (Val): 100%|██████████| 34/34 [00:00<00:00, 83.12it/s, acc=0.498, loss=0.693]\n",
      "Epoch 12 (Train): 100%|██████████| 267/267 [00:06<00:00, 39.79it/s, acc=0.49, loss=0.693] \n",
      "Epoch 12 (Val): 100%|██████████| 34/34 [00:00<00:00, 69.95it/s, acc=0.498, loss=0.693]\n",
      "Epoch 13 (Train): 100%|██████████| 267/267 [00:08<00:00, 33.33it/s, acc=0.492, loss=0.693]\n",
      "Epoch 13 (Val): 100%|██████████| 34/34 [00:00<00:00, 68.35it/s, acc=0.506, loss=0.693]\n",
      "Epoch 14 (Train): 100%|██████████| 267/267 [00:08<00:00, 33.20it/s, acc=0.503, loss=0.693]\n",
      "Epoch 14 (Val): 100%|██████████| 34/34 [00:00<00:00, 71.06it/s, acc=0.498, loss=0.693]\n",
      "Epoch 15 (Train): 100%|██████████| 267/267 [00:07<00:00, 35.17it/s, acc=0.502, loss=0.693]\n",
      "Epoch 15 (Val): 100%|██████████| 34/34 [00:00<00:00, 79.35it/s, acc=0.502, loss=0.693]\n",
      "Epoch 16 (Train): 100%|██████████| 267/267 [00:07<00:00, 35.24it/s, acc=0.489, loss=0.693]\n",
      "Epoch 16 (Val): 100%|██████████| 34/34 [00:00<00:00, 72.17it/s, acc=0.504, loss=0.693]\n",
      "Epoch 17 (Train): 100%|██████████| 267/267 [00:07<00:00, 35.71it/s, acc=0.49, loss=0.693] \n",
      "Epoch 17 (Val): 100%|██████████| 34/34 [00:00<00:00, 79.50it/s, acc=0.498, loss=0.693]\n",
      "Epoch 18 (Train): 100%|██████████| 267/267 [00:06<00:00, 40.30it/s, acc=0.496, loss=0.693]\n",
      "Epoch 18 (Val): 100%|██████████| 34/34 [00:00<00:00, 72.17it/s, acc=0.504, loss=0.693]\n",
      "Epoch 19 (Train): 100%|██████████| 267/267 [00:07<00:00, 36.67it/s, acc=0.501, loss=0.693]\n",
      "Epoch 19 (Val): 100%|██████████| 34/34 [00:00<00:00, 72.99it/s, acc=0.498, loss=0.693]\n",
      "Epoch 20 (Train): 100%|██████████| 267/267 [00:06<00:00, 39.79it/s, acc=0.502, loss=0.693]\n",
      "Epoch 20 (Val): 100%|██████████| 34/34 [00:00<00:00, 60.79it/s, acc=0.498, loss=0.693]\n",
      "Epoch 21 (Train): 100%|██████████| 267/267 [00:06<00:00, 38.26it/s, acc=0.498, loss=0.693]\n",
      "Epoch 21 (Val): 100%|██████████| 34/34 [00:00<00:00, 74.08it/s, acc=0.506, loss=0.693]\n",
      "Epoch 22 (Train): 100%|██████████| 267/267 [00:07<00:00, 34.14it/s, acc=0.497, loss=0.693]\n",
      "Epoch 22 (Val): 100%|██████████| 34/34 [00:00<00:00, 73.76it/s, acc=0.5, loss=0.693]  \n",
      "Epoch 23 (Train): 100%|██████████| 267/267 [00:06<00:00, 38.85it/s, acc=0.501, loss=0.693]\n",
      "Epoch 23 (Val): 100%|██████████| 34/34 [00:00<00:00, 68.00it/s, acc=0.5, loss=0.693]  \n",
      "Epoch 24 (Train): 100%|██████████| 267/267 [00:06<00:00, 38.91it/s, acc=0.501, loss=0.693]\n",
      "Epoch 24 (Val): 100%|██████████| 34/34 [00:00<00:00, 70.32it/s, acc=0.5, loss=0.693]  \n",
      "Epoch 25 (Train): 100%|██████████| 267/267 [00:06<00:00, 43.55it/s, acc=0.49, loss=0.693] \n",
      "Epoch 25 (Val): 100%|██████████| 34/34 [00:00<00:00, 72.41it/s, acc=0.502, loss=0.693]\n",
      "Epoch 26 (Train): 100%|██████████| 267/267 [00:06<00:00, 41.67it/s, acc=0.493, loss=0.693]\n",
      "Epoch 26 (Val): 100%|██████████| 34/34 [00:00<00:00, 70.07it/s, acc=0.502, loss=0.693]\n",
      "Epoch 27 (Train): 100%|██████████| 267/267 [00:08<00:00, 32.67it/s, acc=0.496, loss=0.693]\n",
      "Epoch 27 (Val): 100%|██████████| 34/34 [00:00<00:00, 71.83it/s, acc=0.504, loss=0.693]\n",
      "Epoch 28 (Train): 100%|██████████| 267/267 [00:08<00:00, 32.80it/s, acc=0.498, loss=0.693]\n",
      "Epoch 28 (Val): 100%|██████████| 34/34 [00:00<00:00, 68.74it/s, acc=0.5, loss=0.693]  \n",
      "Epoch 29 (Train): 100%|██████████| 267/267 [00:08<00:00, 32.27it/s, acc=0.503, loss=0.693]\n",
      "Epoch 29 (Val): 100%|██████████| 34/34 [00:00<00:00, 72.58it/s, acc=0.502, loss=0.693]\n",
      "Epoch 30 (Train): 100%|██████████| 267/267 [00:07<00:00, 35.09it/s, acc=0.5, loss=0.693]  \n",
      "Epoch 30 (Val): 100%|██████████| 34/34 [00:00<00:00, 64.64it/s, acc=0.506, loss=0.693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- batch_size_32; lr_0.05; optimizer_SGD; hidden_dim_16; num_layers_2 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Train): 100%|██████████| 267/267 [00:06<00:00, 40.88it/s, acc=0.502, loss=0.694]\n",
      "Epoch 1 (Val): 100%|██████████| 34/34 [00:00<00:00, 80.23it/s, acc=0.502, loss=0.694]\n",
      "Epoch 2 (Train): 100%|██████████| 267/267 [00:06<00:00, 40.53it/s, acc=0.497, loss=0.694]\n",
      "Epoch 2 (Val): 100%|██████████| 34/34 [00:00<00:00, 65.44it/s, acc=0.502, loss=0.693]\n",
      "Epoch 3 (Train): 100%|██████████| 267/267 [00:09<00:00, 29.06it/s, acc=0.49, loss=0.694] \n",
      "Epoch 3 (Val): 100%|██████████| 34/34 [00:00<00:00, 73.82it/s, acc=0.498, loss=0.693]\n",
      "Epoch 4 (Train): 100%|██████████| 267/267 [00:08<00:00, 31.26it/s, acc=0.503, loss=0.693]\n",
      "Epoch 4 (Val): 100%|██████████| 34/34 [00:00<00:00, 78.83it/s, acc=0.502, loss=0.694]\n",
      "Epoch 5 (Train): 100%|██████████| 267/267 [00:06<00:00, 40.43it/s, acc=0.5, loss=0.694]  \n",
      "Epoch 5 (Val): 100%|██████████| 34/34 [00:00<00:00, 79.70it/s, acc=0.496, loss=0.693]\n",
      "Epoch 6 (Train): 100%|██████████| 267/267 [00:06<00:00, 40.31it/s, acc=0.501, loss=0.694]\n",
      "Epoch 6 (Val): 100%|██████████| 34/34 [00:00<00:00, 77.46it/s, acc=0.498, loss=0.693]\n",
      "Epoch 7 (Train): 100%|██████████| 267/267 [00:06<00:00, 39.70it/s, acc=0.501, loss=0.694]\n",
      "Epoch 7 (Val): 100%|██████████| 34/34 [00:00<00:00, 79.20it/s, acc=0.502, loss=0.694]\n",
      "Epoch 8 (Train): 100%|██████████| 267/267 [00:06<00:00, 40.70it/s, acc=0.499, loss=0.694]\n",
      "Epoch 8 (Val): 100%|██████████| 34/34 [00:00<00:00, 78.52it/s, acc=0.496, loss=0.694]\n",
      "Epoch 9 (Train): 100%|██████████| 267/267 [00:06<00:00, 39.44it/s, acc=0.497, loss=0.694]\n",
      "Epoch 9 (Val): 100%|██████████| 34/34 [00:00<00:00, 82.65it/s, acc=0.5, loss=0.695]  \n",
      "Epoch 10 (Train): 100%|██████████| 267/267 [00:06<00:00, 40.33it/s, acc=0.505, loss=0.693]\n",
      "Epoch 10 (Val): 100%|██████████| 34/34 [00:00<00:00, 79.81it/s, acc=0.504, loss=0.695]\n",
      "Epoch 11 (Train): 100%|██████████| 267/267 [00:06<00:00, 39.45it/s, acc=0.505, loss=0.693]\n",
      "Epoch 11 (Val): 100%|██████████| 34/34 [00:00<00:00, 76.86it/s, acc=0.498, loss=0.693]\n",
      "Epoch 12 (Train): 100%|██████████| 267/267 [00:06<00:00, 40.95it/s, acc=0.496, loss=0.694]\n",
      "Epoch 12 (Val): 100%|██████████| 34/34 [00:00<00:00, 76.52it/s, acc=0.5, loss=0.693]  \n",
      "Epoch 13 (Train): 100%|██████████| 267/267 [00:06<00:00, 40.72it/s, acc=0.504, loss=0.694]\n",
      "Epoch 13 (Val): 100%|██████████| 34/34 [00:00<00:00, 78.77it/s, acc=0.498, loss=0.694]\n",
      "Epoch 14 (Train): 100%|██████████| 267/267 [00:06<00:00, 40.64it/s, acc=0.506, loss=0.693]\n",
      "Epoch 14 (Val): 100%|██████████| 34/34 [00:00<00:00, 80.17it/s, acc=0.5, loss=0.693]  \n",
      "Epoch 15 (Train): 100%|██████████| 267/267 [00:06<00:00, 40.54it/s, acc=0.499, loss=0.694]\n",
      "Epoch 15 (Val): 100%|██████████| 34/34 [00:00<00:00, 79.52it/s, acc=0.502, loss=0.693]\n",
      "Epoch 16 (Train): 100%|██████████| 267/267 [00:06<00:00, 40.38it/s, acc=0.499, loss=0.694]\n",
      "Epoch 16 (Val): 100%|██████████| 34/34 [00:00<00:00, 81.98it/s, acc=0.498, loss=0.694]\n",
      "Epoch 17 (Train): 100%|██████████| 267/267 [00:06<00:00, 40.62it/s, acc=0.496, loss=0.694]\n",
      "Epoch 17 (Val): 100%|██████████| 34/34 [00:00<00:00, 85.43it/s, acc=0.5, loss=0.694]  \n",
      "Epoch 18 (Train): 100%|██████████| 267/267 [00:06<00:00, 40.05it/s, acc=0.5, loss=0.694]  \n",
      "Epoch 18 (Val): 100%|██████████| 34/34 [00:00<00:00, 76.37it/s, acc=0.504, loss=0.693]\n",
      "Epoch 19 (Train): 100%|██████████| 267/267 [00:06<00:00, 38.63it/s, acc=0.497, loss=0.694]\n",
      "Epoch 19 (Val): 100%|██████████| 34/34 [00:00<00:00, 79.58it/s, acc=0.492, loss=0.693]\n",
      "Epoch 20 (Train): 100%|██████████| 267/267 [00:06<00:00, 39.33it/s, acc=0.502, loss=0.693]\n",
      "Epoch 20 (Val): 100%|██████████| 34/34 [00:00<00:00, 80.10it/s, acc=0.502, loss=0.694]\n",
      "Epoch 21 (Train): 100%|██████████| 267/267 [00:07<00:00, 34.69it/s, acc=0.505, loss=0.693]\n",
      "Epoch 21 (Val): 100%|██████████| 34/34 [00:00<00:00, 66.11it/s, acc=0.494, loss=0.695]\n",
      "Epoch 22 (Train): 100%|██████████| 267/267 [00:06<00:00, 39.20it/s, acc=0.497, loss=0.694]\n",
      "Epoch 22 (Val): 100%|██████████| 34/34 [00:00<00:00, 81.57it/s, acc=0.5, loss=0.693]  \n",
      "Epoch 23 (Train): 100%|██████████| 267/267 [00:06<00:00, 38.65it/s, acc=0.49, loss=0.694] \n",
      "Epoch 23 (Val): 100%|██████████| 34/34 [00:00<00:00, 80.41it/s, acc=0.502, loss=0.693]\n",
      "Epoch 24 (Train): 100%|██████████| 267/267 [00:06<00:00, 40.55it/s, acc=0.489, loss=0.694]\n",
      "Epoch 24 (Val): 100%|██████████| 34/34 [00:00<00:00, 78.57it/s, acc=0.496, loss=0.693]\n",
      "Epoch 25 (Train): 100%|██████████| 267/267 [00:06<00:00, 40.10it/s, acc=0.498, loss=0.693]\n",
      "Epoch 25 (Val): 100%|██████████| 34/34 [00:00<00:00, 78.60it/s, acc=0.502, loss=0.693]\n",
      "Epoch 26 (Train): 100%|██████████| 267/267 [00:07<00:00, 36.83it/s, acc=0.498, loss=0.694]\n",
      "Epoch 26 (Val): 100%|██████████| 34/34 [00:00<00:00, 79.07it/s, acc=0.498, loss=0.693]\n",
      "Epoch 27 (Train): 100%|██████████| 267/267 [00:06<00:00, 39.82it/s, acc=0.501, loss=0.693]\n",
      "Epoch 27 (Val): 100%|██████████| 34/34 [00:00<00:00, 78.24it/s, acc=0.496, loss=0.693]\n",
      "Epoch 28 (Train): 100%|██████████| 267/267 [00:06<00:00, 39.69it/s, acc=0.495, loss=0.693]\n",
      "Epoch 28 (Val): 100%|██████████| 34/34 [00:00<00:00, 72.77it/s, acc=0.496, loss=0.694]\n",
      "Epoch 29 (Train): 100%|██████████| 267/267 [00:06<00:00, 39.71it/s, acc=0.499, loss=0.693]\n",
      "Epoch 29 (Val): 100%|██████████| 34/34 [00:00<00:00, 79.93it/s, acc=0.504, loss=0.693]\n",
      "Epoch 30 (Train): 100%|██████████| 267/267 [00:06<00:00, 39.70it/s, acc=0.5, loss=0.694]  \n",
      "Epoch 30 (Val): 100%|██████████| 34/34 [00:00<00:00, 76.78it/s, acc=0.496, loss=0.693]\n",
      "Epoch 31 (Train): 100%|██████████| 267/267 [00:06<00:00, 39.57it/s, acc=0.499, loss=0.694]\n",
      "Epoch 31 (Val): 100%|██████████| 34/34 [00:00<00:00, 75.39it/s, acc=0.506, loss=0.693]\n",
      "Epoch 32 (Train): 100%|██████████| 267/267 [00:06<00:00, 40.65it/s, acc=0.498, loss=0.694]\n",
      "Epoch 32 (Val): 100%|██████████| 34/34 [00:00<00:00, 81.04it/s, acc=0.496, loss=0.693]\n",
      "Epoch 33 (Train): 100%|██████████| 267/267 [00:06<00:00, 38.72it/s, acc=0.504, loss=0.693]\n",
      "Epoch 33 (Val): 100%|██████████| 34/34 [00:00<00:00, 60.66it/s, acc=0.496, loss=0.694]\n",
      "Epoch 34 (Train): 100%|██████████| 267/267 [00:07<00:00, 36.56it/s, acc=0.496, loss=0.694]\n",
      "Epoch 34 (Val): 100%|██████████| 34/34 [00:00<00:00, 48.19it/s, acc=0.502, loss=0.693]\n",
      "Epoch 35 (Train): 100%|██████████| 267/267 [00:07<00:00, 33.45it/s, acc=0.5, loss=0.694]  \n",
      "Epoch 35 (Val): 100%|██████████| 34/34 [00:00<00:00, 75.67it/s, acc=0.496, loss=0.693]\n",
      "Epoch 36 (Train): 100%|██████████| 267/267 [00:06<00:00, 39.37it/s, acc=0.492, loss=0.693]\n",
      "Epoch 36 (Val): 100%|██████████| 34/34 [00:00<00:00, 77.98it/s, acc=0.502, loss=0.693]\n",
      "Epoch 37 (Train): 100%|██████████| 267/267 [00:06<00:00, 38.19it/s, acc=0.5, loss=0.694]  \n",
      "Epoch 37 (Val): 100%|██████████| 34/34 [00:00<00:00, 76.13it/s, acc=0.506, loss=0.693]\n",
      "Epoch 38 (Train): 100%|██████████| 267/267 [00:08<00:00, 31.57it/s, acc=0.497, loss=0.694]\n",
      "Epoch 38 (Val): 100%|██████████| 34/34 [00:00<00:00, 69.59it/s, acc=0.502, loss=0.693]\n",
      "Epoch 39 (Train): 100%|██████████| 267/267 [00:07<00:00, 33.89it/s, acc=0.494, loss=0.694]\n",
      "Epoch 39 (Val): 100%|██████████| 34/34 [00:00<00:00, 64.98it/s, acc=0.502, loss=0.694]\n",
      "Epoch 40 (Train): 100%|██████████| 267/267 [00:07<00:00, 35.66it/s, acc=0.506, loss=0.694]\n",
      "Epoch 40 (Val): 100%|██████████| 34/34 [00:00<00:00, 64.02it/s, acc=0.502, loss=0.693]\n",
      "Epoch 41 (Train): 100%|██████████| 267/267 [00:08<00:00, 31.26it/s, acc=0.498, loss=0.693]\n",
      "Epoch 41 (Val): 100%|██████████| 34/34 [00:00<00:00, 65.17it/s, acc=0.498, loss=0.693]\n",
      "Epoch 42 (Train):  37%|███▋      | 99/267 [00:02<00:04, 34.57it/s, acc=0.498, loss=0.694]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m current_model_id \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---------- batch_size_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; lr_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; optimizer_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimizer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; hidden_dim_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; num_layers_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ----------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m model, configuration_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_rnn_model_with_parameters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m  \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m  \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m  \u001b[49m\u001b[43moptimizer_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m  \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m  \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_layers\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m configuration_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m current_model_id\n\u001b[0;32m     23\u001b[0m all_configurations_results\u001b[38;5;241m.\u001b[39mappend(configuration_results)\n",
      "Cell \u001b[1;32mIn[4], line 67\u001b[0m, in \u001b[0;36mtrain_rnn_model_with_parameters\u001b[1;34m(batch_size, learning_rate, optimizer_name, hidden_dim, num_layers)\u001b[0m\n\u001b[0;32m     62\u001b[0m val_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m########################\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m######### Train ########\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m########################\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m model, avg_train_loss, avg_train_acc, avg_val_loss, avg_val_acc, num_of_epochs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_rnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m  \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m  \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m  \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmin_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmax_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m########################\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m######### Plot #########\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m########################\u001b[39;00m\n\u001b[0;32m     80\u001b[0m subtitle \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; lr_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; optimizer_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimizer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; hidden_dim_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; num_layers_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Toh Jing Qiang\\Desktop\\sc4002-nlp-sentiment-classification\\solver.py:42\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, criterion, optimizer, train_dataloader, val_dataloader, min_epoch, max_epoch, max_non_increasing_epoch_count)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     40\u001b[0m     num_of_epochs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 42\u001b[0m     epoch_train_loss, epoch_train_acc \u001b[38;5;241m=\u001b[39m \u001b[43m__train_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     epoch_val_loss, epoch_val_acc \u001b[38;5;241m=\u001b[39m __validate(model, criterion, val_dataloader, epoch)\n\u001b[0;32m     45\u001b[0m     avg_train_loss\u001b[38;5;241m.\u001b[39mappend(epoch_train_loss)\n",
      "File \u001b[1;32mc:\\Users\\Toh Jing Qiang\\Desktop\\sc4002-nlp-sentiment-classification\\solver.py:84\u001b[0m, in \u001b[0;36m__train_one_epoch\u001b[1;34m(model, train_dataloader, optimizer, criterion, epoch)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (sentences, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m     83\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 84\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(predictions, labels)\n\u001b[0;32m     87\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Toh Jing Qiang\\Desktop\\sc4002-nlp-sentiment-classification\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Toh Jing Qiang\\Desktop\\sc4002-nlp-sentiment-classification\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Toh Jing Qiang\\Desktop\\sc4002-nlp-sentiment-classification\\models\\RNN.py:28\u001b[0m, in \u001b[0;36mRNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     26\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[1;32m---> 28\u001b[0m     output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# use last hidden state as sentence representation\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     sentence_representation \u001b[38;5;241m=\u001b[39m hidden[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Toh Jing Qiang\\Desktop\\sc4002-nlp-sentiment-classification\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Toh Jing Qiang\\Desktop\\sc4002-nlp-sentiment-classification\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Toh Jing Qiang\\Desktop\\sc4002-nlp-sentiment-classification\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:714\u001b[0m, in \u001b[0;36mRNN.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRNN_TANH\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 714\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn_tanh\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    725\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    726\u001b[0m         result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mrnn_relu(\n\u001b[0;32m    727\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    728\u001b[0m             hx,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    735\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first,\n\u001b[0;32m    736\u001b[0m         )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_configurations_results = []\n",
    "rnn_models = {}\n",
    "\n",
    "current_model_id = 0\n",
    "\n",
    "for hidden_dim in SEARCH_SPACE[\"hidden_dim\"]:\n",
    "  for num_layers in SEARCH_SPACE[\"num_layers\"]:\n",
    "    for optimizer_name in SEARCH_SPACE[\"optimizer_name\"]:\n",
    "      for batch_size in SEARCH_SPACE[\"batch_size\"]:\n",
    "        for learning_rate in SEARCH_SPACE[\"learning_rate\"]:\n",
    "          current_model_id += 1\n",
    "\n",
    "          print(f\"---------- batch_size_{batch_size}; lr_{learning_rate}; optimizer_{optimizer_name}; hidden_dim_{hidden_dim}; num_layers_{num_layers} ----------\")\n",
    "          model, configuration_results = train_rnn_model_with_parameters(\n",
    "            batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            optimizer_name=optimizer_name,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=num_layers\n",
    "          )\n",
    "          configuration_results[\"model_id\"] = current_model_id\n",
    "\n",
    "          all_configurations_results.append(configuration_results)\n",
    "          rnn_models[current_model_id] = model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Configurations Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>optimizer_name</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>num_of_epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>0.010</td>\n",
       "      <td>SGD</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.693214</td>\n",
       "      <td>0.500286</td>\n",
       "      <td>0.693145</td>\n",
       "      <td>0.506066</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>0.001</td>\n",
       "      <td>SGD</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.693179</td>\n",
       "      <td>0.495552</td>\n",
       "      <td>0.693145</td>\n",
       "      <td>0.502022</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_id  batch_size  learning_rate optimizer_name  hidden_dim  num_layers  \\\n",
       "0         2          32          0.010            SGD          16           2   \n",
       "1         1          32          0.001            SGD          16           2   \n",
       "\n",
       "   train_loss  train_accuracy  val_loss  val_accuracy  num_of_epochs  \n",
       "0    0.693214        0.500286  0.693145      0.506066             30  \n",
       "1    0.693179        0.495552  0.693145      0.502022             29  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_configurations_results_df = pd.DataFrame.from_dict(all_configurations_results)\n",
    "model_configurations_results_df.sort_values(by=[\"val_accuracy\"], ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a. Final Configuration of best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>optimizer_name</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>num_of_epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>0.001</td>\n",
       "      <td>SGD</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.693179</td>\n",
       "      <td>0.495552</td>\n",
       "      <td>0.693145</td>\n",
       "      <td>0.502022</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_id  batch_size  learning_rate optimizer_name  hidden_dim  num_layers  \\\n",
       "0         1          32          0.001            SGD          16           2   \n",
       "\n",
       "   train_loss  train_accuracy  val_loss  val_accuracy  num_of_epochs  \n",
       "0    0.693179        0.495552  0.693145      0.502022             29  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rnn_model_configuration = model_configurations_results_df.head(1)\n",
    "best_rnn_model_configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(16332, 300)\n",
       "  (rnn): RNN(300, 16, num_layers=2, batch_first=True)\n",
       "  (relu): ReLU()\n",
       "  (fc): Linear(in_features=16, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rnn_model_id = best_rnn_model_configuration[\"model_id\"][0]\n",
    "best_rnn_model = rnn_models[best_rnn_model_id]\n",
    "best_rnn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Export Best Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>optimizer_name</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>num_of_epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>0.001</td>\n",
       "      <td>SGD</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.693179</td>\n",
       "      <td>0.495552</td>\n",
       "      <td>0.693145</td>\n",
       "      <td>0.502022</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_id  batch_size  learning_rate optimizer_name  hidden_dim  num_layers  \\\n",
       "0         1          32          0.001            SGD          16           2   \n",
       "\n",
       "   train_loss  train_accuracy  val_loss  val_accuracy  num_of_epochs  \n",
       "0    0.693179        0.495552  0.693145      0.502022             29  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rnn_model_configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving object to local...\n",
      "Object saved to local!\n"
     ]
    }
   ],
   "source": [
    "from utils.file import save_to_local_file\n",
    "\n",
    "model_name = f\"batch_size_{batch_size}-lr_{learning_rate}-optimizer_{optimizer_name}-hidden_dim_{hidden_dim}-num_layers_{num_layers}\"\n",
    "save_to_local_file(f\"models/rnn/{model_name}.pckl\", best_rnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b. Accuracy on Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6931437\n",
      "Test Accuracy: 0.50093806\n"
     ]
    }
   ],
   "source": [
    "from solver import test\n",
    "\n",
    "########################\n",
    "######## Dataset #######\n",
    "########################\n",
    "test_dataset = TextDataset(\n",
    "  dataframe=test_df,\n",
    "  max_len=test_df[\"text\"].str.split().apply(len).max(),\n",
    "  embedding_matrix_vocab_to_index=embedding_matrix_train_dataset_vocab_to_index\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False)\n",
    "\n",
    "########################\n",
    "######### Train ########\n",
    "########################\n",
    "test_loss, test_accuracy = test(\n",
    "  model=best_rnn_model,\n",
    "  criterion=nn.CrossEntropyLoss(),\n",
    "  test_dataloader=test_dataloader,\n",
    ")\n",
    "\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c. Strategies to derive final sentence representation\n",
    "\n",
    "1. Last State: Use last hidden state as sentence representation\n",
    "2. Max Pooling\n",
    "3. Average Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
