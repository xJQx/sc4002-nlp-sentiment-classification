{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Dataset Preparation for OOV Words\n",
    "\n",
    "This notebook handles the import of Word2Vec model and the corresponding logic to build an embedding matrix handling OOV words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Word2Vec Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "import os\n",
    "\n",
    "w2v_model_path = \"models/word2vec-google-news-300\"\n",
    "\n",
    "# Download pretrained embeddings model if haven't done so\n",
    "if not os.path.exists(w2v_model_path):\n",
    "    # Takes around 7mins\n",
    "    model = gensim.downloader.load(\"word2vec-google-news-300\")\n",
    "    model.save(w2v_model_path)\n",
    "\n",
    "    # Alternatively, download from the link below\n",
    "    # w2v_model = gensim.models.KeyedVectors.load_word2vec_format('model\\GoogleNews-vectors-negative300.bin\\GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    # download the pretrained model from https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g (take around 1.5GB)\n",
    "\n",
    "model = gensim.models.KeyedVectors.load(w2v_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Check Word2Vec number of data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.index_to_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juinlee/Documents/GitHub/sc4002-nlp-sentiment-classification/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /Users/juinlee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/juinlee/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/juinlee/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "python(35618) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "from utils.text import tokenize\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "train_dataset = tokenize(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Word Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.text import get_context_average_embedding\n",
    "\n",
    "vocab_train = set([word for sentence in train_dataset[\"tokens\"] for word in sentence])\n",
    "oov_words = set([word for word in vocab_train if word not in model])\n",
    "\n",
    "embedding_dim = model.vector_size\n",
    "\n",
    "# Initialize embedding matrix with zeros (Add 2 for <PAD> and <UNK>).\n",
    "offset = 2\n",
    "embedding_matrix = np.zeros((len(vocab_train) + offset, embedding_dim))\n",
    "\n",
    "# Word to index dictionary for easy lookup.\n",
    "index_from_word = {word: i + offset for i, word in enumerate(vocab_train)}\n",
    "index_from_word[\"<PAD>\"] = 0\n",
    "index_from_word[\"<UNK>\"] = 1\n",
    "\n",
    "# Mean vector of the pretrained GloVe embeddings.\n",
    "vectors = np.array([model[vocab] for vocab in model.index_to_key])\n",
    "mean_vector = np.mean(vectors, axis=0)\n",
    "\n",
    "# Populate embedding matrix with known words.\n",
    "for word, i in index_from_word.items():\n",
    "    if word in model:\n",
    "        embedding_matrix[i] = model[word]\n",
    "\n",
    "\n",
    "# Populate OOV words with context average embedding.\n",
    "oov_words_map = {}\n",
    "for sentence in train_dataset[\"tokens\"]:\n",
    "    for word in sentence:\n",
    "        if word in oov_words:\n",
    "            if word not in oov_words_map:\n",
    "                oov_words_map[word] = 0\n",
    "            oov_words_map[word] += 1\n",
    "            embedding_matrix[index_from_word[word]] += get_context_average_embedding(word, sentence, model)\n",
    "\n",
    "for word in oov_words_map:\n",
    "    embedding_matrix[index_from_word[word]] /= oov_words_map[word]\n",
    "            \n",
    "embedding_matrix[0] = mean_vector\n",
    "embedding_matrix[1] = mean_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Word2Vec Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "embedding_path = Path(\"models/embedding_matrix_oov.npy\")\n",
    "index_from_word_path = Path(\"models/index_from_word_oov.json\")\n",
    "\n",
    "np.save(embedding_path, embedding_matrix)\n",
    "\n",
    "with index_from_word_path.open(\"w\") as f:\n",
    "    json.dump(index_from_word, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
